# Deep Learning for Glaucoma: Detection, Segmentation, Progression Monitoring, and Fairness Analysis

## Table of Contents

1.  [Introduction](#introduction)
2.  [Project Goals](#project-goals)
3.  [Core Tasks](#core-tasks)
4.  [Project Structure](#project-structure)
5.  [Datasets](#datasets)
6.  [Setup and Installation](#setup-and-installation)
7.  [Configuration](#configuration)
8.  [Usage](#usage)
    *   [1. Data Preprocessing](#1-data-preprocessing)
    *   [2. Training Models](#2-training-models)
    *   [3. Feature Extraction (for Progression)](#3-feature-extraction-for-progression)
    *   [4. Training Progression Model](#4-training-progression-model)
    *   [5. Evaluation](#5-evaluation)
    *   [6. Visualization](#6-visualization)
9.  [Models](#models)
10. [Evaluation Metrics](#evaluation-metrics)
11. [Fairness Analysis](#fairness-analysis)
12. [Results](#results)
13. [Future Work](#future-work)
14. [Contributing](#contributing)
15. [License](#license)
16. [Acknowledgements](#acknowledgements)

## Introduction

Glaucoma is a leading cause of irreversible blindness worldwide. It is a progressive optic neuropathy characterized by damage to the optic nerve head (ONH) and corresponding visual field loss. Early detection and continuous monitoring are crucial for managing the disease and preventing severe vision impairment. Fundus photography is a common imaging modality used in glaucoma screening and management.

This project aims to develop and evaluate deep learning models for various tasks related to glaucoma analysis using retinal fundus images. It focuses on:

*   **Automated Detection:** Classifying images as glaucomatous or normal.
*   **Quantitative Analysis:** Segmenting key anatomical structures like the Optic Disc (OD) and Optic Cup (OC) to derive clinically relevant biomarkers (e.g., Cup-to-Disc Ratio - CDR).
*   **Progression Monitoring:** Analyzing longitudinal sequences of fundus images to detect or predict disease progression over time.
*   **Responsible AI:** Assessing the fairness of developed models across different demographic groups (e.g., based on sex or age) where data is available.

The codebase is structured for modularity and reproducibility, facilitating research and experimentation with different models and datasets.

## Project Goals

*   Implement robust pipelines for glaucoma classification, segmentation, and progression feature extraction.
*   Utilize and process multiple open-source fundus image datasets.
*   Train and evaluate deep learning models (CNNs like ResNet, U-Net) for the defined tasks.
*   Develop methods for longitudinal analysis based on derived metrics from segmentation.
*   Incorporate fairness evaluation into the model assessment workflow.
*   Provide a structured and reusable codebase for glaucoma research.

## Core Tasks

This project implements the following core functionalities:

1.  **Glaucoma Classification:** Binary classification of fundus images into 'Normal' (0) and 'Glaucoma' (1).
2.  **Optic Disc & Cup Segmentation:** Multi-task segmentation using a U-Net architecture to delineate the OD and OC boundaries simultaneously.
3.  **Longitudinal Feature Extraction:** Processing sequences of images for individual subjects/eyes, performing segmentation (using a trained model or mock data), calculating clinically relevant metrics (CDR, ISNT rule, areas, etc.) using `src.features.metrics.GlaucomaMetrics`, and summarizing changes over time.
4.  **Progression Prediction:** Training models (e.g., RandomForest) on the extracted longitudinal features to predict clinical progression status.
5.  **Fairness Assessment:** Evaluating classification/progression models for performance disparities across specified sensitive attribute groups (e.g., sex, age bins) using metrics like Equalized Odds and Demographic Parity.

## Project Structure

The project follows a standard machine learning project structure:

```
glaucoma_research_project/
├── configs/                  # Configuration files (YAML) for paths, HPs, models
├── data/                     # Raw and processed data (managed separately if large)
│   ├── raw/                  # Original downloaded datasets
│   └── processed/            # Processed metadata, features, etc.
├── logs/                     # Log files generated by scripts
├── models/                   # Saved trained models (e.g., .joblib for sklearn)
├── notebooks/                # Jupyter notebooks for EDA, visualization, experiments
├── results/                  # Saved evaluation metrics (JSON), plots (PNG), etc.
├── scripts/                  # Executable Python scripts for workflows
│   ├── preprocess_data.py    # Loads & preprocesses raw data
│   ├── train_classification.py # Trains classification model
│   ├── train_segmentation.py   # Trains segmentation model
│   ├── extract_features.py     # Runs segmentation & extracts metrics for progression
│   ├── train_progression.py    # Trains progression model on features
│   ├── evaluate_model.py       # Evaluates trained models
│   └── run_visualization.py    # Generates specific plots
│
├── src/                      # Source code modules
│   ├── data/                 # Data loading, Dataset classes, transforms
│   ├── evaluation/           # Evaluation metric functions (standard & fairness)
│   ├── features/             # Feature calculation (GlaucomaMetrics), feature building
│   ├── models/               # Model architecture definitions (ResNet, UNet, RF)
│   ├── training/             # Training loops, loss functions
│   ├── analysis/             # Progression analysis logic (placeholder)
│   └── utils/                # Utilities (config loading, logging, plotting, file ops)
│
├── tests/                    # Unit and integration tests (TODO)
├── .gitignore                # Git ignore configuration
├── LICENSE                   # Project license file (e.g., MIT) - Choose one!
├── README.md                 # This file
└── requirements.txt          # Python package dependencies
```

## Datasets

This project is designed to work with multiple open-source datasets. The initial implementation includes support for loading and preprocessing:

*   **GRAPE:** Longitudinal dataset with clinical progression labels. ([Link or Reference if available]) - Used for progression monitoring.
*   **SMDG-19:** Contains fundus images with OD/OC segmentations and glaucoma labels. ([Link or Reference]) - Used for segmentation and classification.
*   **AIROGS:** Large dataset for glaucoma classification. ([Link or Reference]) - Used for classification.
*   **(Add others as needed)**

**Note:** Raw dataset files are typically large and are **not** included in this repository. You need to download them separately and place them in the `data/raw/` directory according to the expected structure (e.g., `data/raw/GRAPE/`, `data/raw/SMDG-19/`). Update the paths in `configs/data_paths.yaml` accordingly.

The `scripts/preprocess_data.py` script handles loading metadata, merging relevant information (e.g., GRAPE images and clinical data), cleaning labels, and generating full paths, saving processed metadata files (e.g., `.csv`) to `data/processed/`.

## Setup and Installation

1.  **Prerequisites:**
    *   Python (>= 3.8 recommended)
    *   `pip` and `venv` (or `conda`)

2.  **Clone Repository:**
    ```bash
    git clone https://github.com/your-username/glaucoma_research_project.git # Replace with your repo URL
    cd glaucoma_research_project
    ```

3.  **Create Virtual Environment (Recommended):**
    ```bash
    python -m venv venv
    source venv/bin/activate  # Linux/macOS
    # venv\Scripts\activate    # Windows
    ```
    Or using conda:
    ```bash
    conda create -n glaucoma_env python=3.9 # Or your preferred version
    conda activate glaucoma_env
    ```

4.  **Install Dependencies:**
    ```bash
    pip install -r requirements.txt
    ```
    *Note: This includes PyTorch, Torchvision, Scikit-learn, Pandas, OpenCV, Matplotlib, Seaborn, PyYAML, etc.*
    *If using PyTorch with GPU support, ensure you install the correct version matching your CUDA toolkit: [https://pytorch.org/](https://pytorch.org/)*

5.  **Download Datasets:**
    *   Obtain the GRAPE, SMDG-19, AIROGS (and any other datasets you intend to use) from their respective sources.
    *   Place the raw data files under the `data/raw/` directory, maintaining a consistent structure (e.g., `data/raw/SMDG-19/metadata - standardized.csv`, `data/raw/SMDG-19/full-fundus/full-fundus/...`).

6.  **Configure Paths:**
    *   Edit the YAML files in the `configs/` directory, especially `configs/data_paths.yaml`.
    *   Update `raw_data_dir`, `processed_data_dir`, `log_dir`, `results_dir`, `checkpoint_dir`, `model_save_dir`, `features_dir` to match your system's paths.

7.  **(Optional) External Dependencies:**
    *   If using external tools like the `PVBM.DiscSegmenter` mentioned in the original code, ensure it is installed and accessible in your environment. Update `src/features/external/` if necessary.

## Configuration

Project parameters, hyperparameters, paths, and model settings are managed via YAML files in the `configs/` directory. This allows for easy modification without changing the source code.

*   **`data_paths.yaml`:** Defines root directories for raw data, processed data, logs, results, checkpoints, etc.
*   **`training_config.yaml`:** Contains hyperparameters for different tasks (classification, segmentation, progression) such as learning rate, batch size, epochs, optimizer type, scheduler settings, loss parameters (e.g., `loss_alpha` for segmentation).
*   **`model_config.yaml`:** Specifies model architecture choices and parameters (e.g., ResNet pretrained status, U-Net bilinear mode, number of classes, image size).
*   **`evaluation_config.yaml`:** Parameters for the evaluation script (e.g., batch size, test data file).
*   **`preprocessing_config.yaml`:** Settings for the `preprocess_data.py` script (e.g., which datasets to process, specific file names).
*   **`visualization_config.yaml`:** Settings for the `run_visualization.py` script.

The scripts load these configurations using functions in `src/utils/config_loader.py`.

## Usage

The primary workflows are executed via scripts in the `scripts/` directory. Ensure your virtual environment is activated and you are in the project's root directory. All scripts accept a `--config` argument pointing to your main configuration file (you might have one central config importing others, or pass specific ones).

### 1. Data Preprocessing

Run this script first to prepare the metadata and file paths from the raw datasets.

```bash
python scripts/preprocess_data.py --config configs/main_config.yaml # Replace with your main config path
```

This script reads settings from the `preprocessing` section of your config, loads raw data based on `paths`, processes it using functions from `src.data.data_loading`, and saves cleaned metadata/path files to the `processed_data_dir` specified in your config.

### 2. Training Models

Train specific models using their respective scripts.

*   **Classification (e.g., ResNet):**
    ```bash
    python scripts/train_classification.py --config configs/main_config.yaml [--device cuda]
    ```
    This loads data specified in the `training.classification` config section, initializes a model from `src.models.classification`, uses the trainer from `src.training.trainers`, and saves the best model checkpoint (`best_*.pth`) to `checkpoint_dir` and training history/plots to `results_dir`.

*   **Segmentation (e.g., MultiTaskUNet):**
    ```bash
    python scripts/train_segmentation.py --config configs/main_config.yaml [--device cuda]
    ```
    Similar to classification, but uses segmentation datasets, models (`src.models.segmentation`), loss functions (`src.training.losses`), and trainer logic. Saves checkpoints and results.

### 3. Feature Extraction (for Progression)

After training a segmentation model, use it to extract features (metrics) from longitudinal data (e.g., GRAPE).

```bash
python scripts/extract_features.py \
    --config configs/main_config.yaml \
    --model_checkpoint path/to/your/best_segmentation_model.pth \
    [--device cuda] \
    [--use_mock] # Optional: use mock segmentation instead of model
```

This script:
*   Loads longitudinal data (specified in `feature_extraction` config).
*   Loads the specified segmentation model checkpoint.
*   Uses `src.features.build_features.build_progression_features` to:
    *   Iterate through subject/eye sequences.
    *   Run segmentation on each image (or use mock data).
    *   Calculate metrics using `src.features.metrics.GlaucomaMetrics`.
    *   Analyze changes using logic (ideally from `src.analysis`).
*   Saves the resulting feature DataFrame (e.g., `progression_features.csv`) to `features_dir`.

### 4. Training Progression Model

Train a model (e.g., RandomForest) on the features extracted in the previous step.

```bash
python scripts/train_progression.py --config configs/main_config.yaml
```

This script:
*   Loads the feature file specified in the `training.progression` config.
*   Initializes a model (e.g., from `src.models.progression`).
*   Uses `src.training.trainers.train_sklearn_progression_model` to train and evaluate the model.
*   Saves the trained model (e.g., `.joblib`) to `model_save_dir` and evaluation results (JSON) to `results_dir`.

### 5. Evaluation

Evaluate a previously trained model on a test set.

```bash
python scripts/evaluate_model.py \
    --config configs/main_config.yaml \
    --task_type [classification|segmentation|progression] \
    --model_path path/to/your/model_file.[pth|joblib] \
    [--device cuda]
    # Add --test_data_file or --test_feature_file if not specified in config
```

This script:
*   Loads the specified model (`.pth` for PyTorch, `.joblib` for sklearn).
*   Loads the appropriate test data (images or features).
*   Runs predictions.
*   Calculates metrics using functions from `src.evaluation.metrics`.
*   If sensitive attributes are available and configured for classification/progression tasks, it can also compute fairness metrics.
*   Saves the evaluation results (JSON) to `results_dir`.

### 6. Visualization

Generate specific plots using results or data.

```bash
# Example: Visualize dataset samples
python scripts/run_visualization.py \
    --config configs/main_config.yaml \
    --type samples \
    --dataset_name smdg # Assumes 'smdg' is defined in config['datasets']
    --num_samples 10

# Example: Visualize segmentation predictions
python scripts/run_visualization.py \
    --config configs/main_config.yaml \
    --type segmentation_preds \
    --model_path path/to/your/best_segmentation_model.pth \
    --num_samples 5

# Example: Visualize progression sequence for one subject/eye
python scripts/run_visualization.py \
    --config configs/main_config.yaml \
    --type progression_sequence \
    --subject_id 101 \
    --laterality OD \
    [--segmentation_model path/to/segmentation_model.pth] # Optional overlay
```

This script calls plotting functions from `src.utils.plotting` based on the `--type` argument and saves plots to the `results/visualizations/` directory.

## Models

The following model architectures are implemented:

*   **Classification:** ResNet-50 (`src.models.classification.resnet`) with a modified final layer, leveraging pretrained ImageNet weights.
*   **Segmentation:** Multi-task U-Net (`src.models.segmentation.unet`) with a shared encoder and separate decoders for Optic Cup and Optic Disc segmentation. Supports bilinear upsampling or transpose convolutions.
*   **Progression:** Currently uses Scikit-learn's RandomForestClassifier (`src.models.progression.basic_rf`) as a baseline, trained on extracted features. Can be extended with sequence models (RNNs, LSTMs, Transformers) acting directly on images or features.

## Evaluation Metrics

The project calculates various metrics stored in `src/evaluation/metrics.py`:

*   **Classification:**
    *   Accuracy
    *   Precision
    *   Recall (Sensitivity)
    *   F1-Score
    *   Specificity (True Negative Rate)
    *   ROC AUC (Area Under the Receiver Operating Characteristic Curve)
    *   Confusion Matrix (`[[TN, FP], [FN, TP]]`)
*   **Segmentation:**
    *   Dice Coefficient (F1 Score for segmentation)
    *   Intersection over Union (IoU / Jaccard Index)
    *   Pixel Accuracy
*   **Fairness (for Classification/Progression):** See next section.

## Fairness Analysis

Recognizing the importance of equity in medical AI, this project includes functionality to assess model fairness, particularly for classification and potentially progression tasks where demographic data (e.g., sex, age group) is available for a subset of the data (like SMDG-19).

The `src.evaluation.metrics.calculate_fairness_metrics` function calculates:

*   **Per-Group Performance:** Standard classification metrics computed independently for each subgroup defined by a sensitive attribute (e.g., accuracy for 'Male' vs. accuracy for 'Female').
*   **Disparity Metrics:** Differences in performance between the best- and worst-performing groups:
    *   **Accuracy Difference**
    *   **Recall Difference** (Sensitivity Difference)
    *   **Specificity Difference**
    *   **FPR Difference** (False Positive Rate Difference)
    *   **FNR Difference** (False Negative Rate Difference)
    *   **Equalized Odds Difference:** `max(|TPR_group1 - TPR_group2|, |FPR_group1 - FPR_group2|)` across all group pairs (approximated by `max(TPR_difference, FPR_difference)`). Aims for similar true positive and false positive rates across groups.
    *   **Demographic Parity Difference:** Difference in the rate at which different groups receive the positive prediction (`positive_prediction_rate_difference`).

Fairness analysis can be triggered within the `scripts/evaluate_model.py` script if the test dataset includes the sensitive attribute column and the configuration enables it.

*Note: Age often needs to be binned into groups (e.g., '<50', '50-70', '>70') before being used as a sensitive attribute.*

## Results

*   **Trained Models:** Saved model checkpoints (`.pth` for PyTorch) are stored in the directory specified by `checkpoint_dir` in the config (default: `checkpoints/`). Saved sklearn models (`.joblib`) are stored in `model_save_dir` (default: `models/`).
*   **Evaluation Metrics:** Detailed evaluation results (including standard and fairness metrics) are saved as JSON files in the directory specified by `results_dir` (default: `results/`).
*   **Training History:** Logs and JSON files containing epoch-wise loss/metrics are saved in `results_dir`.
*   **Visualizations:** Plots generated by `run_visualization.py` are saved in `results/visualizations/`.
*   **Logs:** Detailed execution logs for each script run are saved in the directory specified by `log_dir` (default: `logs/`).
*   **Features:** Extracted features for progression analysis are saved in `features_dir` (default: `data/processed/features/`).

## Future Work

*   Integrate more datasets (e.g., REFUGE, Drishti-GS).
*   Implement more advanced segmentation models (e.g., Attention U-Net, TransUNet).
*   Develop sequence models (RNN, LSTM, Transformer) for end-to-end progression monitoring directly from images or combined with features.
*   Expand fairness analysis (more metrics, intersectional fairness, algorithmic bias mitigation).
*   Implement comprehensive unit and integration tests (`tests/`).
*   Add support for multi-class classification if needed.
*   Explore self-supervised learning for pretraining.
*   Refactor `analyze_progression` logic from placeholder to a robust implementation in `src/analysis/`.
*   Improve configuration management (e.g., using Hydra).
*   Package the project for easier installation and distribution.

## Contributing

Contributions are welcome! Please follow standard practices like creating issues for bugs or feature requests and submitting pull requests for code changes. (Add more specific contribution guidelines if desired).

## License

(Choose and specify a license). Example:
This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Acknowledgements

*   Mention the providers of the open-source datasets used (GRAPE, SMDG-19, AIROGS, etc.).
*   Acknowledge any libraries or tools heavily relied upon.
*   Thank collaborators or funding sources if applicable.
```