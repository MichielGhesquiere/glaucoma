#!/usr/bin/env python3
"""
Fairness Analysis Script - Generate additional insights from fairness evaluation results.

This script loads the results from evaluate_fairness.py and generates additional
analysis and visualizations for fairness assessment.
"""

import argparse
import json
import logging
import os
import sys
from pathlib import Path
from typing import Dict, List, Any

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
from scipy import stats

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class FairnessAnalyzer:
    """Analyze fairness evaluation results and generate insights."""
    
    def __init__(self, results_dir: str, output_dir: str):
        self.results_dir = Path(results_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Load results
        self.results = self._load_results()
        
    def _load_results(self) -> List[Dict]:
        """Load fairness evaluation results."""
        results_file = self.results_dir / 'fairness_evaluation_results.json'
        
        if not results_file.exists():
            raise FileNotFoundError(f"Results file not found: {results_file}")
            
        with open(results_file, 'r') as f:
            return json.load(f)
    
    def generate_fairness_report(self):
        """Generate comprehensive fairness report."""
        logger.info("Generating fairness report...")
        
        # Create report sections
        report_sections = []
        
        # 1. Executive Summary
        report_sections.append(self._create_executive_summary())
        
        # 2. Overall Performance Summary
        report_sections.append(self._create_performance_summary())
        
        # 3. Demographic Analysis
        report_sections.append(self._create_demographic_analysis())
        
        # 4. Camera Type Analysis (for CHAKSU)
        report_sections.append(self._create_camera_analysis())
        
        # 5. Fairness Metrics Summary
        report_sections.append(self._create_fairness_metrics_summary())
        
        # 6. Recommendations
        report_sections.append(self._create_recommendations())
        
        # Combine into full report
        full_report = "\n\n".join(report_sections)
        
        # Save report
        report_file = self.output_dir / 'fairness_analysis_report.md'
        with open(report_file, 'w') as f:
            f.write(full_report)
            
        logger.info(f"Fairness report saved to: {report_file}")\n        \n    def _create_executive_summary(self) -> str:\n        \"\"\"Create executive summary of fairness evaluation.\"\"\"\n        n_models = len(set(r['model_name'] for r in self.results))\n        n_datasets = len(set(r['dataset_name'] for r in self.results))\n        total_samples = sum(r['n_samples'] for r in self.results)\n        \n        # Find worst fairness violations\n        max_demo_parity_diff = 0\n        worst_model = None\n        worst_dataset = None\n        \n        for result in self.results:\n            for metric_name, values in result['fairness_metrics'].items():\n                if 'demographic_parity' in metric_name and isinstance(values, dict):\n                    diff = values.get('difference', 0)\n                    if diff > max_demo_parity_diff:\n                        max_demo_parity_diff = diff\n                        worst_model = result['model_name']\n                        worst_dataset = result['dataset_name']\n        \n        summary = f\"\"\"# Fairness Evaluation Report\n\n## Executive Summary\n\nThis report presents the results of a comprehensive fairness evaluation across {n_models} model(s) and {n_datasets} dataset(s), covering a total of {total_samples:,} samples.\n\n### Key Findings:\n\n- **Models Evaluated**: {n_models}\n- **Datasets Analyzed**: {n_datasets}\n- **Total Samples**: {total_samples:,}\n- **Largest Demographic Parity Violation**: {max_demo_parity_diff:.3f} ({worst_model} on {worst_dataset})\n\n### Fairness Assessment:\n\n{'ðŸ”´ **SIGNIFICANT FAIRNESS CONCERNS**' if max_demo_parity_diff > 0.1 else 'ðŸŸ¡ **MODERATE FAIRNESS CONCERNS**' if max_demo_parity_diff > 0.05 else 'ðŸŸ¢ **GOOD FAIRNESS PROFILE**'}\n\nThe largest difference in positive prediction rates between demographic groups is {max_demo_parity_diff:.1%}, which {'exceeds' if max_demo_parity_diff > 0.1 else 'approaches' if max_demo_parity_diff > 0.05 else 'is within'} commonly accepted fairness thresholds.\"\"\"\n        \n        return summary\n    \n    def _create_performance_summary(self) -> str:\n        \"\"\"Create overall performance summary.\"\"\"\n        # Calculate performance statistics\n        performance_data = []\n        \n        for result in self.results:\n            performance_data.append({\n                'Model': result['model_name'],\n                'Dataset': result['dataset_name'],\n                'AUC': result['overall_metrics'].get('auc', np.nan),\n                'Accuracy': result['overall_metrics'].get('accuracy', np.nan),\n                'N_Samples': result['n_samples']\n            })\n            \n        df = pd.DataFrame(performance_data)\n        \n        # Calculate summary statistics\n        summary_stats = df.groupby('Model').agg({\n            'AUC': ['mean', 'std', 'min', 'max'],\n            'Accuracy': ['mean', 'std', 'min', 'max'],\n            'N_Samples': 'sum'\n        }).round(3)\n        \n        summary = f\"\"\"## Overall Performance Summary\n\n### Performance by Model\n\n{summary_stats.to_markdown()}\n\n### Performance Insights:\n\n\"\"\"\n        \n        # Add insights based on performance\n        best_auc_model = df.loc[df['AUC'].idxmax(), 'Model'] if not df['AUC'].isna().all() else \"N/A\"\n        best_acc_model = df.loc[df['Accuracy'].idxmax(), 'Model'] if not df['Accuracy'].isna().all() else \"N/A\"\n        \n        summary += f\"- **Best AUC**: {best_auc_model}\\n\"\n        summary += f\"- **Best Accuracy**: {best_acc_model}\\n\"\n        \n        # Check for performance consistency\n        auc_cv = df.groupby('Model')['AUC'].std() / df.groupby('Model')['AUC'].mean()\n        most_consistent = auc_cv.idxmin() if not auc_cv.isna().all() else \"N/A\"\n        summary += f\"- **Most Consistent Performance**: {most_consistent}\\n\"\n        \n        return summary\n    \n    def _create_demographic_analysis(self) -> str:\n        \"\"\"Create demographic analysis section.\"\"\"\n        summary = \"## Demographic Analysis\\n\\n\"\n        \n        # Analyze age groups\n        age_analysis = self._analyze_demographic_attribute('age_group')\n        if age_analysis:\n            summary += \"### Age Group Analysis\\n\\n\"\n            summary += age_analysis + \"\\n\\n\"\n            \n        # Analyze sex/gender\n        sex_analysis = self._analyze_demographic_attribute('sex_clean')\n        if sex_analysis:\n            summary += \"### Sex/Gender Analysis\\n\\n\"\n            summary += sex_analysis + \"\\n\\n\"\n            \n        # Analyze ethnicity\n        ethnicity_analysis = self._analyze_demographic_attribute('ethnicity_clean')\n        if ethnicity_analysis:\n            summary += \"### Ethnicity Analysis\\n\\n\"\n            summary += ethnicity_analysis + \"\\n\\n\"\n            \n        return summary\n    \n    def _analyze_demographic_attribute(self, attribute: str) -> str:\n        \"\"\"Analyze a specific demographic attribute across all results.\"\"\"\n        findings = []\n        \n        for result in self.results:\n            if attribute not in result.get('subgroup_metrics', {}):\n                continue\n                \n            subgroup_data = result['subgroup_metrics'][attribute]\n            model_name = result['model_name']\n            dataset_name = result['dataset_name']\n            \n            # Find performance gaps\n            aucs = [metrics['auc'] for metrics in subgroup_data.values() \n                   if metrics and metrics.get('auc') is not None]\n            \n            if len(aucs) >= 2:\n                auc_gap = max(aucs) - min(aucs)\n                best_group = max(subgroup_data.items(), \n                               key=lambda x: x[1]['auc'] if x[1] and x[1].get('auc') else 0)[0]\n                worst_group = min(subgroup_data.items(), \n                                key=lambda x: x[1]['auc'] if x[1] and x[1].get('auc') else 1)[0]\n                \n                findings.append(f\"- **{model_name} on {dataset_name}**: AUC gap of {auc_gap:.3f} \"\n                              f\"(Best: {best_group}, Worst: {worst_group})\")\n                \n        if not findings:\n            return f\"No sufficient data available for {attribute} analysis.\"\n            \n        analysis = f\"Performance gaps identified in {attribute}:\\n\\n\"\n        analysis += \"\\n\".join(findings)\n        \n        return analysis\n    \n    def _create_camera_analysis(self) -> str:\n        \"\"\"Create camera type analysis for CHAKSU dataset.\"\"\"\n        summary = \"## Camera Type Analysis (CHAKSU Dataset)\\n\\n\"\n        \n        chaksu_results = [r for r in self.results if r['dataset_name'] == 'CHAKSU']\n        \n        if not chaksu_results:\n            return summary + \"No CHAKSU dataset results available for camera analysis.\\n\"\n            \n        camera_findings = []\n        \n        for result in chaksu_results:\n            camera_metrics = result.get('subgroup_metrics', {}).get('camera_type', {})\n            \n            if not camera_metrics:\n                continue\n                \n            model_name = result['model_name']\n            \n            # Analyze camera performance\n            camera_aucs = {}\n            for camera_group, metrics in camera_metrics.items():\n                if metrics and metrics.get('auc') is not None:\n                    camera_name = camera_group.replace('camera_type_', '')\n                    camera_aucs[camera_name] = metrics['auc']\n                    \n            if len(camera_aucs) >= 2:\n                best_camera = max(camera_aucs.items(), key=lambda x: x[1])\n                worst_camera = min(camera_aucs.items(), key=lambda x: x[1])\n                auc_gap = best_camera[1] - worst_camera[1]\n                \n                camera_findings.append(\n                    f\"- **{model_name}**: {auc_gap:.3f} AUC gap between cameras \"\n                    f\"(Best: {best_camera[0]} - {best_camera[1]:.3f}, \"\n                    f\"Worst: {worst_camera[0]} - {worst_camera[1]:.3f})\"\n                )\n                \n        if camera_findings:\n            summary += \"Camera-specific performance differences:\\n\\n\"\n            summary += \"\\n\".join(camera_findings)\n        else:\n            summary += \"No significant camera-specific performance differences detected.\"\n            \n        return summary\n    \n    def _create_fairness_metrics_summary(self) -> str:\n        \"\"\"Create fairness metrics summary.\"\"\"\n        summary = \"## Fairness Metrics Summary\\n\\n\"\n        \n        # Collect demographic parity violations\n        demo_parity_violations = []\n        \n        for result in self.results:\n            for metric_name, values in result['fairness_metrics'].items():\n                if 'demographic_parity' in metric_name and isinstance(values, dict):\n                    diff = values.get('difference', 0)\n                    if diff > 0.05:  # Threshold for concern\n                        demo_parity_violations.append({\n                            'Model': result['model_name'],\n                            'Dataset': result['dataset_name'],\n                            'Attribute': metric_name.replace('_demographic_parity', ''),\n                            'Difference': diff\n                        })\n                        \n        if demo_parity_violations:\n            summary += \"### Demographic Parity Violations (>5% difference)\\n\\n\"\n            violations_df = pd.DataFrame(demo_parity_violations)\n            summary += violations_df.to_markdown(index=False) + \"\\n\\n\"\n        else:\n            summary += \"### âœ… No significant demographic parity violations detected\\n\\n\"\n            \n        # Add fairness threshold interpretation\n        summary += \"\"\"### Fairness Threshold Interpretation\n\n- **Demographic Parity**: Difference in positive prediction rates\n  - < 5%: Acceptable\n  - 5-10%: Concerning\n  - > 10%: Significant violation\n\n- **Equalized Odds**: Difference in TPR/FPR across groups\n  - < 5%: Acceptable\n  - 5-10%: Concerning  \n  - > 10%: Significant violation\n\"\"\"\n        \n        return summary\n    \n    def _create_recommendations(self) -> str:\n        \"\"\"Create recommendations based on fairness analysis.\"\"\"\n        recommendations = []\n        \n        # Check for overall fairness issues\n        max_demo_parity = 0\n        for result in self.results:\n            for metric_name, values in result['fairness_metrics'].items():\n                if 'demographic_parity' in metric_name and isinstance(values, dict):\n                    diff = values.get('difference', 0)\n                    max_demo_parity = max(max_demo_parity, diff)\n                    \n        if max_demo_parity > 0.1:\n            recommendations.append(\n                \"ðŸ”´ **CRITICAL**: Implement bias mitigation techniques such as \"\n                \"adversarial debiasing, fairness constraints, or data rebalancing.\"\n            )\n        elif max_demo_parity > 0.05:\n            recommendations.append(\n                \"ðŸŸ¡ **MODERATE**: Monitor fairness metrics closely and consider \"\n                \"implementing fairness-aware training techniques.\"\n            )\n        else:\n            recommendations.append(\n                \"ðŸŸ¢ **GOOD**: Current fairness profile is acceptable, but continue monitoring.\"\n            )\n            \n        # Check for data representation issues\n        small_groups = 0\n        for result in self.results:\n            for attr_name, subgroups in result.get('subgroup_metrics', {}).items():\n                for group_name, metrics in subgroups.items():\n                    if metrics and metrics.get('n_samples', 0) < 50:\n                        small_groups += 1\n                        \n        if small_groups > 0:\n            recommendations.append(\n                f\"ðŸ“Š **DATA**: {small_groups} demographic subgroups have <50 samples. \"\n                \"Consider collecting additional data for underrepresented groups.\"\n            )\n            \n        # Camera-specific recommendations\n        chaksu_results = [r for r in self.results if r['dataset_name'] == 'CHAKSU']\n        if chaksu_results:\n            camera_gaps = []\n            for result in chaksu_results:\n                camera_metrics = result.get('subgroup_metrics', {}).get('camera_type', {})\n                if camera_metrics:\n                    aucs = [m['auc'] for m in camera_metrics.values() \n                           if m and m.get('auc') is not None]\n                    if len(aucs) >= 2:\n                        camera_gaps.append(max(aucs) - min(aucs))\n                        \n            if camera_gaps and max(camera_gaps) > 0.1:\n                recommendations.append(\n                    \"ðŸ“· **DEVICE**: Significant performance differences across camera types. \"\n                    \"Consider domain adaptation techniques or camera-specific calibration.\"\n                )\n                \n        summary = \"## Recommendations\\n\\n\"\n        if recommendations:\n            summary += \"\\n\\n\".join(f\"{i+1}. {rec}\" for i, rec in enumerate(recommendations))\n        else:\n            summary += \"No specific recommendations at this time. Continue monitoring fairness metrics.\"\n            \n        return summary\n    \n    def create_summary_visualizations(self):\n        \"\"\"Create summary visualizations.\"\"\"\n        logger.info(\"Creating summary visualizations...\")\n        \n        # 1. Fairness heatmap\n        self._create_fairness_heatmap()\n        \n        # 2. Performance vs fairness scatter plot\n        self._create_performance_fairness_scatter()\n        \n        # 3. Subgroup sample size distribution\n        self._create_sample_size_distribution()\n        \n    def _create_fairness_heatmap(self):\n        \"\"\"Create heatmap of fairness metrics across models and datasets.\"\"\"\n        # Collect demographic parity data\n        heatmap_data = []\n        \n        for result in self.results:\n            row_data = {\n                'Model': result['model_name'],\n                'Dataset': result['dataset_name']\n            }\n            \n            # Extract demographic parity differences\n            for metric_name, values in result['fairness_metrics'].items():\n                if 'demographic_parity' in metric_name and isinstance(values, dict):\n                    attr_name = metric_name.replace('_demographic_parity', '')\n                    row_data[f'{attr_name}_parity'] = values.get('difference', 0)\n                    \n            heatmap_data.append(row_data)\n            \n        if not heatmap_data:\n            return\n            \n        df = pd.DataFrame(heatmap_data)\n        \n        # Create pivot table for heatmap\n        parity_cols = [col for col in df.columns if col.endswith('_parity')]\n        if not parity_cols:\n            return\n            \n        # Create separate heatmaps for each parity metric\n        for parity_col in parity_cols:\n            if df[parity_col].isna().all():\n                continue\n                \n            pivot_df = df.pivot(index='Model', columns='Dataset', values=parity_col)\n            \n            plt.figure(figsize=(10, 6))\n            sns.heatmap(pivot_df, annot=True, fmt='.3f', cmap='RdYlGn_r', \n                       center=0.05, vmin=0, vmax=0.2)\n            plt.title(f'Demographic Parity Differences: {parity_col}')\n            plt.tight_layout()\n            plt.savefig(self.output_dir / f'fairness_heatmap_{parity_col}.png', \n                       dpi=300, bbox_inches='tight')\n            plt.close()\n            \n    def _create_performance_fairness_scatter(self):\n        \"\"\"Create scatter plot of performance vs fairness.\"\"\"\n        scatter_data = []\n        \n        for result in self.results:\n            # Get overall AUC\n            auc = result['overall_metrics'].get('auc', np.nan)\n            \n            # Get maximum demographic parity difference\n            max_parity_diff = 0\n            for metric_name, values in result['fairness_metrics'].items():\n                if 'demographic_parity' in metric_name and isinstance(values, dict):\n                    diff = values.get('difference', 0)\n                    max_parity_diff = max(max_parity_diff, diff)\n                    \n            scatter_data.append({\n                'Model': result['model_name'],\n                'Dataset': result['dataset_name'],\n                'AUC': auc,\n                'Max_Parity_Diff': max_parity_diff\n            })\n            \n        df = pd.DataFrame(scatter_data)\n        \n        if df['AUC'].isna().all() or df['Max_Parity_Diff'].isna().all():\n            return\n            \n        plt.figure(figsize=(10, 8))\n        \n        # Color by model\n        models = df['Model'].unique()\n        colors = plt.cm.Set1(np.linspace(0, 1, len(models)))\n        \n        for i, model in enumerate(models):\n            model_data = df[df['Model'] == model]\n            plt.scatter(model_data['Max_Parity_Diff'], model_data['AUC'], \n                       c=[colors[i]], label=model, s=100, alpha=0.7)\n            \n        # Add fairness threshold lines\n        plt.axvline(x=0.05, color='orange', linestyle='--', alpha=0.7, \n                   label='5% Parity Threshold')\n        plt.axvline(x=0.10, color='red', linestyle='--', alpha=0.7, \n                   label='10% Parity Threshold')\n                   \n        plt.xlabel('Maximum Demographic Parity Difference')\n        plt.ylabel('AUC')\n        plt.title('Performance vs Fairness Trade-off')\n        plt.legend()\n        plt.grid(True, alpha=0.3)\n        plt.tight_layout()\n        plt.savefig(self.output_dir / 'performance_vs_fairness.png', \n                   dpi=300, bbox_inches='tight')\n        plt.close()\n        \n    def _create_sample_size_distribution(self):\n        \"\"\"Create visualization of sample sizes across subgroups.\"\"\"\n        sample_data = []\n        \n        for result in self.results:\n            for attr_name, subgroups in result.get('subgroup_metrics', {}).items():\n                for group_name, metrics in subgroups.items():\n                    if metrics and metrics.get('n_samples'):\n                        sample_data.append({\n                            'Model': result['model_name'],\n                            'Dataset': result['dataset_name'],\n                            'Attribute': attr_name,\n                            'Group': group_name,\n                            'N_Samples': metrics['n_samples']\n                        })\n                        \n        if not sample_data:\n            return\n            \n        df = pd.DataFrame(sample_data)\n        \n        # Create box plot of sample sizes by attribute\n        plt.figure(figsize=(12, 8))\n        sns.boxplot(data=df, x='Attribute', y='N_Samples')\n        plt.yscale('log')\n        plt.xticks(rotation=45)\n        plt.title('Distribution of Sample Sizes Across Demographic Groups')\n        plt.ylabel('Number of Samples (log scale)')\n        plt.tight_layout()\n        plt.savefig(self.output_dir / 'sample_size_distribution.png', \n                   dpi=300, bbox_inches='tight')\n        plt.close()\n\n\ndef main():\n    parser = argparse.ArgumentParser(\n        description=\"Analyze fairness evaluation results and generate insights\"\n    )\n    \n    parser.add_argument('--results_dir', type=str, required=True,\n                       help='Directory containing fairness evaluation results')\n    parser.add_argument('--output_dir', type=str, default='fairness_analysis',\n                       help='Directory to save analysis outputs')\n    \n    args = parser.parse_args()\n    \n    try:\n        analyzer = FairnessAnalyzer(args.results_dir, args.output_dir)\n        \n        # Generate report\n        analyzer.generate_fairness_report()\n        \n        # Create visualizations\n        analyzer.create_summary_visualizations()\n        \n        logger.info(f\"Fairness analysis completed. Results saved to: {args.output_dir}\")\n        \n    except Exception as e:\n        logger.error(f\"Fairness analysis failed: {e}\", exc_info=True)\n        return 1\n        \n    return 0\n\n\nif __name__ == \"__main__\":\n    exit(main())\n
