#!/usr/bin/env python3
"""
Fairness Evaluation Script for Multi-Source Domain Adaptation Models.

This script evaluates trained models for fairness across demographic subgroups
and other sensitive attributes available in the datasets (e.g., age, sex, camera type).
It computes various fairness metrics including demographic parity, equalized odds,
and underdiagnosis disparities.
"""

import argparse
import json
import logging
import os
import sys
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
import torch
import torch.nn.functional as F
from sklearn.metrics import roc_auc_score, accuracy_score, roc_curve
from torch.utils.data import DataLoader
from tqdm import tqdm

# Add project root to path
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

# Import custom modules
from src.data.datasets import GlaucomaSubgroupDataset, safe_collate
from src.data.external_loader import load_external_test_data
from src.data.transforms import get_transforms
from src.data.utils import get_eval_transforms
from src.evaluation.fairness import analyze_fairness, calculate_underdiagnosis_disparities
from src.models.classification.build_model import build_classifier_model
from src.training.domain_adaptation import compute_ece, sensitivity_at_specificity
from src.utils.helpers import set_seed


class FairnessEvaluator:
    """Main class for evaluating model fairness across different demographic groups."""
    
    def __init__(self, args):
        self.args = args
        self.logger = logging.getLogger(__name__)
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # Create output directory
        self.output_dir = Path(args.output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Setup results storage
        self.results = {}
        
    def load_datasets_with_metadata(self) -> Dict[str, pd.DataFrame]:
        """Load datasets ensuring metadata columns are preserved."""
        self.logger.info("Loading datasets with metadata for fairness analysis...")
        
        # Use external loader to get datasets
        datasets = load_external_test_data(
            smdg_metadata_file_raw=self.args.smdg_metadata_file_raw,
            smdg_image_dir_raw=self.args.smdg_image_dir_raw,
            chaksu_base_dir_eval=self.args.chaksu_base_dir,
            chaksu_decision_dir_raw=self.args.chaksu_decision_dir_raw,
            chaksu_metadata_dir_raw=self.args.chaksu_metadata_dir_raw,
            data_type=self.args.data_type,
            base_data_root=self.args.base_data_root,
            raw_dir_name='raw',
            processed_dir_name='processed',
            eval_papilla=self.args.eval_papilla,
            eval_oiaodir_test=False,
            eval_chaksu=self.args.eval_chaksu,
            eval_acrima=self.args.eval_acrima,
            eval_hygd=self.args.eval_hygd,
            acrima_image_dir_raw=self.args.acrima_image_dir_raw,
            hygd_image_dir_raw=self.args.hygd_image_dir_raw,
            hygd_labels_file_raw=self.args.hygd_labels_file_raw
        )\n        \n        # Load SMDG-19 manually to preserve all metadata\n        if self.args.eval_smdg:\n            try:\n                smdg_metadata_file = os.path.join(self.args.base_data_root, self.args.smdg_metadata_file_raw)\n                smdg_image_dir = os.path.join(self.args.base_data_root, self.args.smdg_image_dir_raw)\n                \n                if os.path.exists(smdg_metadata_file):\n                    df_smdg = pd.read_csv(smdg_metadata_file)\n                    \n                    # Add image paths and filter existing files\n                    df_smdg[\"image_path\"] = df_smdg[\"names\"].apply(\n                        lambda name: os.path.join(smdg_image_dir, f\"{name}.png\")\n                    )\n                    df_smdg[\"file_exists\"] = df_smdg[\"image_path\"].apply(os.path.exists)\n                    df_smdg = df_smdg[df_smdg[\"file_exists\"]]\n                    \n                    # Clean labels and ensure required columns\n                    df_smdg = df_smdg.dropna(subset=[\"types\"])\n                    df_smdg = df_smdg[df_smdg[\"types\"].isin([0, 1])]\n                    df_smdg[\"types\"] = df_smdg[\"types\"].astype(int)\n                    df_smdg[\"label\"] = df_smdg[\"types\"]  # Add compatibility\n                    df_smdg[\"dataset_source\"] = \"SMDG-19\"\n                    \n                    # Process demographic information if available\n                    self._process_smdg_demographics(df_smdg)\n                    \n                    if not df_smdg.empty:\n                        datasets['SMDG-19'] = df_smdg\n                        self.logger.info(f\"Loaded SMDG-19: {len(df_smdg)} samples with metadata\")\n                        \n            except Exception as e:\n                self.logger.error(f\"Error loading SMDG-19: {e}\")\n        \n        # Process metadata for each dataset\n        for dataset_name, df in datasets.items():\n            self._process_dataset_metadata(df, dataset_name)\n            \n        # Log available metadata columns for each dataset\n        for dataset_name, df in datasets.items():\n            metadata_cols = self._get_available_metadata_columns(df)\n            self.logger.info(f\"{dataset_name} available metadata: {metadata_cols}\")\n            \n        return datasets\n    \n    def _process_smdg_demographics(self, df: pd.DataFrame):\n        \"\"\"Process SMDG-19 specific demographic information.\"\"\"\n        # Age processing\n        if 'age' in df.columns:\n            df['age_numeric'] = pd.to_numeric(df['age'], errors='coerce')\n            df['age_group'] = df['age_numeric'].apply(self._categorize_age)\n            \n        # Sex processing\n        if 'sex' in df.columns:\n            df['sex_clean'] = df['sex'].str.upper().replace({'M': 'Male', 'F': 'Female'})\n            \n        # Ethnicity processing (if available)\n        if 'ethnicity' in df.columns:\n            df['ethnicity_clean'] = df['ethnicity'].fillna('Unknown')\n            \n    def _process_dataset_metadata(self, df: pd.DataFrame, dataset_name: str):\n        \"\"\"Process metadata columns for a specific dataset.\"\"\"\n        # Standardize camera information for CHAKSU\n        if dataset_name == 'CHAKSU' and 'camera' in df.columns:\n            df['camera_type'] = df['camera'].fillna('Unknown')\n            \n        # Process age if available\n        if 'age' in df.columns:\n            df['age_numeric'] = pd.to_numeric(df['age'], errors='coerce')\n            df['age_group'] = df['age_numeric'].apply(self._categorize_age)\n            \n        # Process sex if available\n        if 'sex' in df.columns or 'gender' in df.columns:\n            sex_col = 'sex' if 'sex' in df.columns else 'gender'\n            df['sex_clean'] = df[sex_col].str.upper().replace({'M': 'Male', 'F': 'Female'})\n            \n        # Add dataset source as a potential grouping variable\n        if 'dataset_source' not in df.columns:\n            df['dataset_source'] = dataset_name\n            \n    def _categorize_age(self, age):\n        \"\"\"Categorize age into meaningful groups.\"\"\"\n        if pd.isna(age):\n            return 'Unknown'\n        if age < 50:\n            return '<50'\n        elif age < 65:\n            return '50-64'\n        elif age < 75:\n            return '65-74'\n        else:\n            return '75+'\n    \n    def _get_available_metadata_columns(self, df: pd.DataFrame) -> List[str]:\n        \"\"\"Get list of metadata columns available for fairness analysis.\"\"\"\n        potential_metadata = [\n            'age', 'age_numeric', 'age_group',\n            'sex', 'sex_clean', 'gender',\n            'camera', 'camera_type',\n            'ethnicity', 'ethnicity_clean',\n            'dataset_source'\n        ]\n        return [col for col in potential_metadata if col in df.columns]\n    \n    def discover_trained_models(self) -> List[Dict[str, Any]]:\n        \"\"\"Discover trained models from experiment directories.\"\"\"\n        models = []\n        \n        # Look for model checkpoints in experiment directories\n        experiment_dir = Path(self.args.experiment_parent_dir)\n        \n        if not experiment_dir.exists():\n            self.logger.error(f\"Experiment directory not found: {experiment_dir}\")\n            return models\n            \n        # Search for model checkpoints\n        for model_dir in experiment_dir.glob('*'):\n            if not model_dir.is_dir():\n                continue\n                \n            checkpoint_dir = model_dir / 'checkpoints'\n            if not checkpoint_dir.exists():\n                continue\n                \n            # Look for .pth files\n            checkpoint_files = list(checkpoint_dir.glob('*.pth'))\n            if not checkpoint_files:\n                continue\n                \n            # Take the best or latest checkpoint\n            best_checkpoint = None\n            for ckpt in checkpoint_files:\n                if 'best' in ckpt.name.lower():\n                    best_checkpoint = ckpt\n                    break\n            if not best_checkpoint:\n                best_checkpoint = sorted(checkpoint_files)[-1]  # Latest by name\n                \n            # Extract model info from directory name\n            model_info = self._parse_model_info(model_dir.name)\n            model_info['checkpoint_path'] = str(best_checkpoint)\n            model_info['experiment_dir'] = str(model_dir)\n            \n            models.append(model_info)\n            \n        self.logger.info(f\"Discovered {len(models)} trained models\")\n        return models\n    \n    def _parse_model_info(self, dir_name: str) -> Dict[str, Any]:\n        \"\"\"Parse model information from directory name.\"\"\"\n        # This should match the naming convention from your training script\n        # Adjust based on your actual naming scheme\n        parts = dir_name.split('_')\n        \n        model_info = {\n            'name': dir_name,\n            'model_name': 'vit_base_patch16_224',  # Default\n            'custom_weights_path': None\n        }\n        \n        # Try to extract model architecture\n        if 'vit' in dir_name.lower():\n            model_info['model_name'] = 'vit_base_patch16_224'\n        elif 'resnet' in dir_name.lower():\n            if 'resnet50' in dir_name.lower():\n                model_info['model_name'] = 'resnet50'\n            elif 'resnet18' in dir_name.lower():\n                model_info['model_name'] = 'resnet18'\n                \n        # Check for VFM weights\n        if 'vfm' in dir_name.lower() or 'custom' in dir_name.lower():\n            model_info['custom_weights_path'] = self.args.vfm_weights_path\n            \n        return model_info\n    \n    def load_model(self, model_info: Dict[str, Any]) -> torch.nn.Module:\n        \"\"\"Load a trained model from checkpoint.\"\"\"\n        # Build model architecture\n        model = build_classifier_model(\n            model_name=model_info['model_name'],\n            num_classes=self.args.num_classes,\n            custom_weights_path=model_info.get('custom_weights_path')\n        )\n        \n        # Load checkpoint\n        checkpoint_path = model_info['checkpoint_path']\n        try:\n            checkpoint = torch.load(checkpoint_path, map_location=self.device)\n            \n            # Handle different checkpoint formats\n            if 'model_state_dict' in checkpoint:\n                model.load_state_dict(checkpoint['model_state_dict'])\n            elif 'state_dict' in checkpoint:\n                model.load_state_dict(checkpoint['state_dict'])\n            else:\n                model.load_state_dict(checkpoint)\n                \n            self.logger.info(f\"Loaded model from {checkpoint_path}\")\n            \n        except Exception as e:\n            self.logger.error(f\"Error loading model from {checkpoint_path}: {e}\")\n            raise\n            \n        model = model.to(self.device)\n        model.eval()\n        \n        return model\n    \n    def evaluate_model_fairness(self, model: torch.nn.Module, dataset: pd.DataFrame, \n                              dataset_name: str, model_name: str) -> Dict[str, Any]:\n        \"\"\"Evaluate model fairness on a dataset.\"\"\"\n        self.logger.info(f\"Evaluating fairness for {model_name} on {dataset_name}\")\n        \n        # Get evaluation transforms\n        _, eval_transforms = get_transforms(\n            self.args.image_size, \n            'vit_base_patch16_224',  # Use standard transforms\n            use_data_augmentation=False\n        )\n        \n        # Get available metadata columns for this dataset\n        metadata_cols = self._get_available_metadata_columns(dataset)\n        \n        # Create dataset with metadata\n        eval_dataset = GlaucomaSubgroupDataset(\n            dataset,\n            transform=eval_transforms,\n            sensitive_attributes=metadata_cols,\n            require_attributes=False\n        )\n        \n        # Create dataloader\n        eval_loader = DataLoader(\n            eval_dataset,\n            batch_size=self.args.eval_batch_size,\n            shuffle=False,\n            num_workers=self.args.num_workers,\n            collate_fn=safe_collate\n        )\n        \n        # Run inference\n        predictions, probabilities, targets, metadata = self._run_inference(\n            model, eval_loader\n        )\n        \n        # Create results DataFrame\n        results_df = pd.DataFrame({\n            'prediction': predictions,\n            'probability': probabilities,\n            'target': targets\n        })\n        \n        # Add metadata\n        for key, values in metadata.items():\n            results_df[key] = values\n            \n        # Calculate overall metrics\n        overall_metrics = self._calculate_overall_metrics(targets, predictions, probabilities)\n        \n        # Calculate subgroup metrics\n        subgroup_metrics = self._calculate_subgroup_metrics(\n            results_df, metadata_cols\n        )\n        \n        # Calculate fairness metrics\n        fairness_metrics = self._calculate_fairness_metrics(\n            results_df, metadata_cols\n        )\n        \n        # Create visualizations\n        self._create_fairness_visualizations(\n            results_df, metadata_cols, dataset_name, model_name\n        )\n        \n        return {\n            'dataset_name': dataset_name,\n            'model_name': model_name,\n            'overall_metrics': overall_metrics,\n            'subgroup_metrics': subgroup_metrics,\n            'fairness_metrics': fairness_metrics,\n            'n_samples': len(results_df),\n            'available_metadata': metadata_cols\n        }\n    \n    def _run_inference(self, model: torch.nn.Module, dataloader: DataLoader) -> Tuple[List, List, List, Dict]:\n        \"\"\"Run model inference and collect predictions with metadata.\"\"\"\n        predictions = []\n        probabilities = []\n        targets = []\n        metadata = {}\n        \n        with torch.no_grad():\n            for batch in tqdm(dataloader, desc=\"Running inference\"):\n                if batch is None:\n                    continue\n                    \n                images, labels, batch_metadata = batch\n                images = images.to(self.device)\n                \n                # Forward pass\n                outputs = model(images)\n                probs = F.softmax(outputs, dim=1)\n                preds = torch.argmax(outputs, dim=1)\n                \n                # Collect results\n                predictions.extend(preds.cpu().numpy())\n                probabilities.extend(probs[:, 1].cpu().numpy())  # Probability of positive class\n                targets.extend(labels.numpy())\n                \n                # Collect metadata\n                for key, values in batch_metadata.items():\n                    if key not in metadata:\n                        metadata[key] = []\n                    metadata[key].extend(values)\n                    \n        return predictions, probabilities, targets, metadata\n    \n    def _calculate_overall_metrics(self, targets: List, predictions: List, probabilities: List) -> Dict[str, float]:\n        \"\"\"Calculate overall performance metrics.\"\"\"\n        targets = np.array(targets)\n        predictions = np.array(predictions)\n        probabilities = np.array(probabilities)\n        \n        # Filter out invalid targets (e.g., -1 from failed image loads)\n        valid_mask = targets >= 0\n        targets = targets[valid_mask]\n        predictions = predictions[valid_mask]\n        probabilities = probabilities[valid_mask]\n        \n        if len(targets) == 0:\n            return {}\n            \n        accuracy = accuracy_score(targets, predictions)\n        auc = roc_auc_score(targets, probabilities) if len(np.unique(targets)) > 1 else 0.0\n        ece = compute_ece(targets, probabilities)\n        sens_at_95_spec = sensitivity_at_specificity(targets, probabilities, 0.95)\n        \n        return {\n            'accuracy': float(accuracy),\n            'auc': float(auc),\n            'ece': float(ece),\n            'sensitivity_at_95_specificity': float(sens_at_95_spec),\n            'n_samples': len(targets)\n        }\n    \n    def _calculate_subgroup_metrics(self, results_df: pd.DataFrame, metadata_cols: List[str]) -> Dict[str, Dict]:\n        \"\"\"Calculate performance metrics for each subgroup.\"\"\"\n        subgroup_metrics = {}\n        \n        for col in metadata_cols:\n            if col not in results_df.columns:\n                continue\n                \n            # Skip if too many unique values (continuous variables)\n            if results_df[col].nunique() > 20:\n                continue\n                \n            col_metrics = {}\n            \n            for group_value in results_df[col].dropna().unique():\n                group_data = results_df[results_df[col] == group_value]\n                \n                # Skip small groups\n                if len(group_data) < self.args.min_samples_per_group:\n                    continue\n                    \n                # Calculate metrics for this group\n                targets = group_data['target'].values\n                predictions = group_data['prediction'].values\n                probabilities = group_data['probability'].values\n                \n                # Filter valid targets\n                valid_mask = targets >= 0\n                if valid_mask.sum() == 0:\n                    continue\n                    \n                targets = targets[valid_mask]\n                predictions = predictions[valid_mask]\n                probabilities = probabilities[valid_mask]\n                \n                if len(np.unique(targets)) < 2:\n                    # Cannot calculate some metrics with only one class\n                    group_metrics = {\n                        'n_samples': len(targets),\n                        'accuracy': accuracy_score(targets, predictions),\n                        'auc': None,\n                        'tpr': None,\n                        'fpr': None\n                    }\n                else:\n                    # Calculate all metrics\n                    from sklearn.metrics import confusion_matrix\n                    \n                    cm = confusion_matrix(targets, predictions, labels=[0, 1])\n                    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, len(targets))\n                    \n                    tpr = tp / (tp + fn) if (tp + fn) > 0 else 0\n                    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n                    \n                    group_metrics = {\n                        'n_samples': len(targets),\n                        'accuracy': accuracy_score(targets, predictions),\n                        'auc': roc_auc_score(targets, probabilities),\n                        'tpr': tpr,\n                        'fpr': fpr,\n                        'tn': int(tn), 'fp': int(fp), 'fn': int(fn), 'tp': int(tp)\n                    }\n                \n                col_metrics[f\"{col}_{group_value}\"] = group_metrics\n                \n            if col_metrics:\n                subgroup_metrics[col] = col_metrics\n                \n        return subgroup_metrics\n    \n    def _calculate_fairness_metrics(self, results_df: pd.DataFrame, metadata_cols: List[str]) -> Dict[str, Any]:\n        \"\"\"Calculate fairness metrics across subgroups.\"\"\"\n        fairness_metrics = {}\n        \n        for col in metadata_cols:\n            if col not in results_df.columns:\n                continue\n                \n            # Skip if too many unique values\n            if results_df[col].nunique() > 20:\n                continue\n                \n            # Calculate demographic parity (equal positive prediction rates)\n            group_stats = []\n            groups = results_df[col].dropna().unique()\n            \n            for group_value in groups:\n                group_data = results_df[results_df[col] == group_value]\n                \n                if len(group_data) < self.args.min_samples_per_group:\n                    continue\n                    \n                positive_rate = group_data['prediction'].mean()\n                group_stats.append({\n                    'group': f\"{col}_{group_value}\",\n                    'positive_rate': positive_rate,\n                    'n_samples': len(group_data)\n                })\n                \n            if len(group_stats) >= 2:\n                positive_rates = [g['positive_rate'] for g in group_stats]\n                demographic_parity_diff = max(positive_rates) - min(positive_rates)\n                \n                fairness_metrics[f\"{col}_demographic_parity\"] = {\n                    'difference': demographic_parity_diff,\n                    'group_stats': group_stats\n                }\n                \n        return fairness_metrics\n    \n    def _create_fairness_visualizations(self, results_df: pd.DataFrame, metadata_cols: List[str], \n                                      dataset_name: str, model_name: str):\n        \"\"\"Create visualizations for fairness analysis.\"\"\"\n        # Create subdirectory for this model-dataset combination\n        viz_dir = self.output_dir / 'visualizations' / f\"{model_name}_{dataset_name}\"\n        viz_dir.mkdir(parents=True, exist_ok=True)\n        \n        for col in metadata_cols:\n            if col not in results_df.columns or results_df[col].nunique() > 10:\n                continue\n                \n            # Performance by group\n            self._plot_performance_by_group(results_df, col, viz_dir, dataset_name, model_name)\n            \n            # Confusion matrix by group\n            self._plot_confusion_matrices_by_group(results_df, col, viz_dir, dataset_name, model_name)\n            \n    def _plot_performance_by_group(self, results_df: pd.DataFrame, group_col: str, \n                                  output_dir: Path, dataset_name: str, model_name: str):\n        \"\"\"Plot performance metrics by group.\"\"\"\n        fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n        fig.suptitle(f'Performance by {group_col}\\n{model_name} on {dataset_name}', fontsize=14)\n        \n        groups = results_df[group_col].dropna().unique()\n        group_metrics = []\n        \n        for group in groups:\n            group_data = results_df[results_df[group_col] == group]\n            \n            if len(group_data) < self.args.min_samples_per_group:\n                continue\n                \n            targets = group_data['target'].values\n            predictions = group_data['prediction'].values\n            probabilities = group_data['probability'].values\n            \n            # Filter valid targets\n            valid_mask = targets >= 0\n            if valid_mask.sum() == 0:\n                continue\n                \n            targets = targets[valid_mask]\n            predictions = predictions[valid_mask]\n            probabilities = probabilities[valid_mask]\n            \n            accuracy = accuracy_score(targets, predictions)\n            auc = roc_auc_score(targets, probabilities) if len(np.unique(targets)) > 1 else np.nan\n            \n            group_metrics.append({\n                'Group': str(group),\n                'Accuracy': accuracy,\n                'AUC': auc,\n                'N': len(targets)\n            })\n            \n        if not group_metrics:\n            plt.close(fig)\n            return\n            \n        metrics_df = pd.DataFrame(group_metrics)\n        \n        # Plot accuracy\n        axes[0, 0].bar(metrics_df['Group'], metrics_df['Accuracy'])\n        axes[0, 0].set_title('Accuracy by Group')\n        axes[0, 0].set_ylabel('Accuracy')\n        axes[0, 0].tick_params(axis='x', rotation=45)\n        \n        # Plot AUC\n        valid_auc = metrics_df.dropna(subset=['AUC'])\n        if not valid_auc.empty:\n            axes[0, 1].bar(valid_auc['Group'], valid_auc['AUC'])\n            axes[0, 1].set_title('AUC by Group')\n            axes[0, 1].set_ylabel('AUC')\n            axes[0, 1].tick_params(axis='x', rotation=45)\n        \n        # Plot sample sizes\n        axes[1, 0].bar(metrics_df['Group'], metrics_df['N'])\n        axes[1, 0].set_title('Sample Size by Group')\n        axes[1, 0].set_ylabel('Number of Samples')\n        axes[1, 0].tick_params(axis='x', rotation=45)\n        \n        # Plot positive prediction rates\n        pos_rates = []\n        for group in groups:\n            group_data = results_df[results_df[group_col] == group]\n            if len(group_data) >= self.args.min_samples_per_group:\n                pos_rate = group_data['prediction'].mean()\n                pos_rates.append(pos_rate)\n            else:\n                pos_rates.append(np.nan)\n                \n        axes[1, 1].bar(range(len(groups)), pos_rates)\n        axes[1, 1].set_title('Positive Prediction Rate by Group')\n        axes[1, 1].set_ylabel('Positive Prediction Rate')\n        axes[1, 1].set_xticks(range(len(groups)))\n        axes[1, 1].set_xticklabels([str(g) for g in groups], rotation=45)\n        \n        plt.tight_layout()\n        plt.savefig(output_dir / f'performance_by_{group_col}.png', dpi=300, bbox_inches='tight')\n        plt.close()\n        \n    def _plot_confusion_matrices_by_group(self, results_df: pd.DataFrame, group_col: str,\n                                         output_dir: Path, dataset_name: str, model_name: str):\n        \"\"\"Plot confusion matrices for each group.\"\"\"\n        groups = results_df[group_col].dropna().unique()\n        valid_groups = []\n        \n        # Filter groups with sufficient samples\n        for group in groups:\n            group_data = results_df[results_df[group_col] == group]\n            if len(group_data) >= self.args.min_samples_per_group:\n                valid_groups.append(group)\n                \n        if len(valid_groups) == 0:\n            return\n            \n        n_groups = len(valid_groups)\n        cols = min(3, n_groups)\n        rows = (n_groups + cols - 1) // cols\n        \n        fig, axes = plt.subplots(rows, cols, figsize=(4*cols, 4*rows))\n        fig.suptitle(f'Confusion Matrices by {group_col}\\n{model_name} on {dataset_name}', fontsize=14)\n        \n        if n_groups == 1:\n            axes = [axes]\n        elif rows == 1:\n            axes = axes.reshape(1, -1)\n            \n        for idx, group in enumerate(valid_groups):\n            row = idx // cols\n            col = idx % cols\n            ax = axes[row, col] if rows > 1 else axes[col]\n            \n            group_data = results_df[results_df[group_col] == group]\n            targets = group_data['target'].values\n            predictions = group_data['prediction'].values\n            \n            # Filter valid targets\n            valid_mask = targets >= 0\n            targets = targets[valid_mask]\n            predictions = predictions[valid_mask]\n            \n            if len(targets) == 0:\n                continue\n                \n            from sklearn.metrics import confusion_matrix\n            cm = confusion_matrix(targets, predictions, labels=[0, 1])\n            \n            sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap='Blues',\n                       xticklabels=['Normal', 'Glaucoma'],\n                       yticklabels=['Normal', 'Glaucoma'])\n            ax.set_title(f'{group} (n={len(targets)})')\n            ax.set_xlabel('Predicted')\n            ax.set_ylabel('Actual')\n            \n        # Hide unused subplots\n        for idx in range(n_groups, rows * cols):\n            if rows > 1:\n                row = idx // cols\n                col = idx % cols\n                axes[row, col].set_visible(False)\n            elif cols > 1:\n                axes[idx].set_visible(False)\n                \n        plt.tight_layout()\n        plt.savefig(output_dir / f'confusion_matrices_by_{group_col}.png', dpi=300, bbox_inches='tight')\n        plt.close()\n    \n    def save_results(self, all_results: List[Dict]):\n        \"\"\"Save fairness evaluation results.\"\"\"\n        # Save detailed results as JSON\n        results_file = self.output_dir / 'fairness_evaluation_results.json'\n        with open(results_file, 'w') as f:\n            json.dump(all_results, f, indent=2, default=str)\n            \n        # Create summary tables\n        self._create_summary_tables(all_results)\n        \n        # Create cross-dataset comparison\n        self._create_cross_dataset_comparison(all_results)\n        \n    def _create_summary_tables(self, all_results: List[Dict]):\n        \"\"\"Create summary tables for fairness metrics.\"\"\"\n        summary_data = []\n        \n        for result in all_results:\n            base_row = {\n                'Model': result['model_name'],\n                'Dataset': result['dataset_name'],\n                'N_Samples': result['n_samples'],\n                'Overall_AUC': result['overall_metrics'].get('auc', np.nan),\n                'Overall_Accuracy': result['overall_metrics'].get('accuracy', np.nan)\n            }\n            \n            # Add fairness metrics\n            for fairness_metric, values in result['fairness_metrics'].items():\n                if isinstance(values, dict) and 'difference' in values:\n                    base_row[f'{fairness_metric}_difference'] = values['difference']\n                    \n            summary_data.append(base_row)\n            \n        summary_df = pd.DataFrame(summary_data)\n        summary_df.to_csv(self.output_dir / 'fairness_summary.csv', index=False)\n        \n        self.logger.info(f\"Saved fairness summary to {self.output_dir / 'fairness_summary.csv'}\")\n        \n    def _create_cross_dataset_comparison(self, all_results: List[Dict]):\n        \"\"\"Create comparison across datasets and models.\"\"\"\n        # Group results by model\n        models = {}\n        for result in all_results:\n            model_name = result['model_name']\n            if model_name not in models:\n                models[model_name] = []\n            models[model_name].append(result)\n            \n        # Create comparison plots\n        for model_name, model_results in models.items():\n            self._plot_model_fairness_comparison(model_name, model_results)\n            \n    def _plot_model_fairness_comparison(self, model_name: str, model_results: List[Dict]):\n        \"\"\"Plot fairness comparison for a single model across datasets.\"\"\"\n        if len(model_results) < 2:\n            return\n            \n        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n        fig.suptitle(f'Fairness Comparison: {model_name}', fontsize=16)\n        \n        datasets = [r['dataset_name'] for r in model_results]\n        overall_aucs = [r['overall_metrics'].get('auc', np.nan) for r in model_results]\n        overall_accs = [r['overall_metrics'].get('accuracy', np.nan) for r in model_results]\n        \n        # Overall performance comparison\n        axes[0, 0].bar(datasets, overall_aucs)\n        axes[0, 0].set_title('Overall AUC by Dataset')\n        axes[0, 0].set_ylabel('AUC')\n        axes[0, 0].tick_params(axis='x', rotation=45)\n        \n        axes[0, 1].bar(datasets, overall_accs)\n        axes[0, 1].set_title('Overall Accuracy by Dataset')\n        axes[0, 1].set_ylabel('Accuracy')\n        axes[0, 1].tick_params(axis='x', rotation=45)\n        \n        # Demographic parity differences\n        demo_parity_diffs = []\n        for result in model_results:\n            max_diff = 0\n            for metric_name, values in result['fairness_metrics'].items():\n                if 'demographic_parity' in metric_name and isinstance(values, dict):\n                    max_diff = max(max_diff, values.get('difference', 0))\n            demo_parity_diffs.append(max_diff)\n            \n        axes[1, 0].bar(datasets, demo_parity_diffs)\n        axes[1, 0].set_title('Maximum Demographic Parity Difference')\n        axes[1, 0].set_ylabel('Difference in Positive Prediction Rates')\n        axes[1, 0].tick_params(axis='x', rotation=45)\n        \n        # Sample sizes\n        sample_sizes = [r['n_samples'] for r in model_results]\n        axes[1, 1].bar(datasets, sample_sizes)\n        axes[1, 1].set_title('Sample Sizes by Dataset')\n        axes[1, 1].set_ylabel('Number of Samples')\n        axes[1, 1].tick_params(axis='x', rotation=45)\n        \n        plt.tight_layout()\n        plt.savefig(self.output_dir / f'fairness_comparison_{model_name}.png', \n                   dpi=300, bbox_inches='tight')\n        plt.close()\n    \n    def run_fairness_evaluation(self):\n        \"\"\"Run complete fairness evaluation.\"\"\"\n        # Load datasets with metadata\n        datasets = self.load_datasets_with_metadata()\n        \n        if not datasets:\n            self.logger.error(\"No datasets loaded. Exiting.\")\n            return\n            \n        # Discover trained models\n        models = self.discover_trained_models()\n        \n        if not models:\n            self.logger.error(\"No trained models found. Exiting.\")\n            return\n            \n        # Evaluate each model on each dataset\n        all_results = []\n        \n        for model_info in models:\n            self.logger.info(f\"Evaluating model: {model_info['name']}\")\n            \n            try:\n                model = self.load_model(model_info)\n                \n                for dataset_name, dataset_df in datasets.items():\n                    if len(dataset_df) < self.args.min_samples_per_dataset:\n                        self.logger.warning(f\"Skipping {dataset_name}: only {len(dataset_df)} samples\")\n                        continue\n                        \n                    result = self.evaluate_model_fairness(\n                        model, dataset_df, dataset_name, model_info['name']\n                    )\n                    all_results.append(result)\n                    \n            except Exception as e:\n                self.logger.error(f\"Error evaluating model {model_info['name']}: {e}\")\n                continue\n                \n        # Save results\n        if all_results:\n            self.save_results(all_results)\n            self.logger.info(f\"Fairness evaluation completed. Results saved to {self.output_dir}\")\n        else:\n            self.logger.error(\"No results generated.\")\n\n\ndef main():\n    parser = argparse.ArgumentParser(\n        description=\"Fairness Evaluation for Multi-Source Domain Adaptation Models\"\n    )\n    \n    # Data arguments\n    data_group = parser.add_argument_group('Data Configuration')\n    data_group.add_argument('--data_type', type=str, default='raw', \n                           choices=['raw', 'processed'])\n    data_group.add_argument('--base_data_root', type=str, default=r'D:\\glaucoma\\data')\n    \n    # Dataset paths\n    data_group.add_argument('--smdg_metadata_file_raw', type=str, \n                           default=os.path.join('raw','SMDG-19','metadata - standardized.csv'))\n    data_group.add_argument('--smdg_image_dir_raw', type=str, \n                           default=os.path.join('raw','SMDG-19','full-fundus','full-fundus'))\n    data_group.add_argument('--chaksu_base_dir', type=str, \n                           default=os.path.join('raw','Chaksu','Train','Train','1.0_Original_Fundus_Images'))\n    data_group.add_argument('--chaksu_decision_dir_raw', type=str, \n                           default=os.path.join('raw','Chaksu','Train','Train','6.0_Glaucoma_Decision'))\n    data_group.add_argument('--chaksu_metadata_dir_raw', type=str, \n                           default=os.path.join('raw','Chaksu','Train','Train','6.0_Glaucoma_Decision','Majority'))\n    data_group.add_argument('--acrima_image_dir_raw', type=str, \n                           default=os.path.join('raw','ACRIMA','Database','Images'))\n    data_group.add_argument('--hygd_image_dir_raw', type=str, \n                           default=os.path.join('raw','HYGD','HYGD','Images'))\n    data_group.add_argument('--hygd_labels_file_raw', type=str, \n                           default=os.path.join('raw','HYGD','HYGD','Labels.csv'))\n    \n    # Dataset selection\n    data_group.add_argument('--eval_smdg', action='store_true', default=True)\n    data_group.add_argument('--eval_papilla', action='store_true', default=True)\n    data_group.add_argument('--eval_chaksu', action='store_true', default=True)\n    data_group.add_argument('--eval_acrima', action='store_true', default=True)\n    data_group.add_argument('--eval_hygd', action='store_true', default=True)\n    \n    # Model arguments\n    model_group = parser.add_argument_group('Model Configuration')\n    model_group.add_argument('--experiment_parent_dir', type=str, \n                            default='experiments/multisource',\n                            help='Parent directory containing trained model experiments')\n    model_group.add_argument('--vfm_weights_path', type=str,\n                            default='models/VFM_Fundus_weights.pth')\n    model_group.add_argument('--num_classes', type=int, default=2)\n    model_group.add_argument('--image_size', type=int, default=224)\n    \n    # Evaluation arguments\n    eval_group = parser.add_argument_group('Evaluation Configuration')\n    eval_group.add_argument('--eval_batch_size', type=int, default=32)\n    eval_group.add_argument('--num_workers', type=int, default=4)\n    eval_group.add_argument('--min_samples_per_group', type=int, default=10,\n                           help='Minimum samples required per demographic group')\n    eval_group.add_argument('--min_samples_per_dataset', type=int, default=50,\n                           help='Minimum samples required per dataset')\n    \n    # Output arguments\n    output_group = parser.add_argument_group('Output Configuration')\n    output_group.add_argument('--output_dir', type=str, \n                             default='fairness_evaluation_results',\n                             help='Directory to save fairness evaluation results')\n    \n    # System arguments\n    sys_group = parser.add_argument_group('System Configuration')\n    sys_group.add_argument('--seed', type=int, default=42)\n    \n    args = parser.parse_args()\n    \n    # Set random seed\n    set_seed(args.seed)\n    \n    # Setup logging\n    logging.basicConfig(\n        level=logging.INFO,\n        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n    )\n    \n    try:\n        evaluator = FairnessEvaluator(args)\n        evaluator.run_fairness_evaluation()\n        return 0\n    except Exception as e:\n        logging.error(f\"Fairness evaluation failed: {e}\", exc_info=True)\n        return 1\n\n\nif __name__ == \"__main__\":\n    exit(main())\n
