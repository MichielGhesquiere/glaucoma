{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "# Data manipulation and linear algebra\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Machine Learning and Neural Networks\n",
    "import sklearn\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.transforms import (Compose, Normalize, RandomAffine, RandomApply, RandomHorizontalFlip,\n",
    "                                    RandomResizedCrop, RandomRotation, Resize, ToTensor)\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "\n",
    "# Image processing\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import timm\n",
    "from transformers import ViTForImageClassification, ViTImageProcessor\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>names</th>\n",
       "      <th>types</th>\n",
       "      <th>type_expanded</th>\n",
       "      <th>isColor</th>\n",
       "      <th>original_name</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>fundus</th>\n",
       "      <th>fundus_od_seg</th>\n",
       "      <th>fundus_oc_seg</th>\n",
       "      <th>bv_seg</th>\n",
       "      <th>...</th>\n",
       "      <th>cdr_expert4</th>\n",
       "      <th>refractive_dioptre_1</th>\n",
       "      <th>refractive_dioptre_2</th>\n",
       "      <th>refractive_astigmatism</th>\n",
       "      <th>phakic_or_pseudophakic</th>\n",
       "      <th>iop_perkins</th>\n",
       "      <th>iop_pneumatic</th>\n",
       "      <th>pachymetry</th>\n",
       "      <th>axial_length</th>\n",
       "      <th>visual_field_mean_defect</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OIA-ODIR-TEST-OFFLINE-1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1029_right.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/full-fundus/OIA-ODIR-TEST-OFFLINE-1.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OIA-ODIR-TEST-OFFLINE-2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1049_left.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/full-fundus/OIA-ODIR-TEST-OFFLINE-2.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OIA-ODIR-TEST-OFFLINE-4</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1128_left.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/full-fundus/OIA-ODIR-TEST-OFFLINE-4.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OIA-ODIR-TEST-OFFLINE-5</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1179_right.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/full-fundus/OIA-ODIR-TEST-OFFLINE-5.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OIA-ODIR-TEST-OFFLINE-6</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1180_left.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/full-fundus/OIA-ODIR-TEST-OFFLINE-6.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     names  types type_expanded  isColor   original_name  \\\n",
       "0  OIA-ODIR-TEST-OFFLINE-1      0           NaN        1  1029_right.jpg   \n",
       "1  OIA-ODIR-TEST-OFFLINE-2      0           NaN        1   1049_left.jpg   \n",
       "2  OIA-ODIR-TEST-OFFLINE-4      0           NaN        1   1128_left.jpg   \n",
       "3  OIA-ODIR-TEST-OFFLINE-5      0           NaN        1  1179_right.jpg   \n",
       "4  OIA-ODIR-TEST-OFFLINE-6      0           NaN        1   1180_left.jpg   \n",
       "\n",
       "  patient_id                                    fundus fundus_od_seg  \\\n",
       "0        NaN  /full-fundus/OIA-ODIR-TEST-OFFLINE-1.png           NaN   \n",
       "1        NaN  /full-fundus/OIA-ODIR-TEST-OFFLINE-2.png           NaN   \n",
       "2        NaN  /full-fundus/OIA-ODIR-TEST-OFFLINE-4.png           NaN   \n",
       "3        NaN  /full-fundus/OIA-ODIR-TEST-OFFLINE-5.png           NaN   \n",
       "4        NaN  /full-fundus/OIA-ODIR-TEST-OFFLINE-6.png           NaN   \n",
       "\n",
       "  fundus_oc_seg bv_seg  ... cdr_expert4 refractive_dioptre_1  \\\n",
       "0           NaN    NaN  ...         NaN                  NaN   \n",
       "1           NaN    NaN  ...         NaN                  NaN   \n",
       "2           NaN    NaN  ...         NaN                  NaN   \n",
       "3           NaN    NaN  ...         NaN                  NaN   \n",
       "4           NaN    NaN  ...         NaN                  NaN   \n",
       "\n",
       "  refractive_dioptre_2 refractive_astigmatism phakic_or_pseudophakic  \\\n",
       "0                  NaN                    NaN                    NaN   \n",
       "1                  NaN                    NaN                    NaN   \n",
       "2                  NaN                    NaN                    NaN   \n",
       "3                  NaN                    NaN                    NaN   \n",
       "4                  NaN                    NaN                    NaN   \n",
       "\n",
       "  iop_perkins  iop_pneumatic  pachymetry axial_length  \\\n",
       "0         NaN            NaN         NaN          NaN   \n",
       "1         NaN            NaN         NaN          NaN   \n",
       "2         NaN            NaN         NaN          NaN   \n",
       "3         NaN            NaN         NaN          NaN   \n",
       "4         NaN            NaN         NaN          NaN   \n",
       "\n",
       "   visual_field_mean_defect  \n",
       "0                       NaN  \n",
       "1                       NaN  \n",
       "2                       NaN  \n",
       "3                       NaN  \n",
       "4                       NaN  \n",
       "\n",
       "[5 rows x 48 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load in the D:\\glaucoma_datasets\\SMDG-19\\metadata - standardized.csv as a pandas dataframe\n",
    "import pandas as pd\n",
    "\n",
    "# load in the metadata\n",
    "df = pd.read_csv(r'D:\\glaucoma\\data\\raw\\SMDG-19\\metadata - standardized.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['names', 'types', 'type_expanded', 'isColor', 'original_name',\n",
      "       'patient_id', 'fundus', 'fundus_od_seg', 'fundus_oc_seg', 'bv_seg',\n",
      "       'artery_seg', 'vein_seg', 'oct', 'oct_oc_seg', 'oct_od_seg', 'sex',\n",
      "       'gender', 'age', 'eye', 'sbp', 'dbp', 'hr', 'iop', 'vcdr',\n",
      "       'Unnamed: 24', 'notchI_present', 'notchS_present', 'notchN_present',\n",
      "       'notchT_present', 'expert1_grade', 'expert2_grade', 'expert3_grade',\n",
      "       'expert4_grade', 'expert5_grade', 'cdr_avg', 'cdr_expert1',\n",
      "       'cdr_expert2', 'cdr_expert3', 'cdr_expert4', 'refractive_dioptre_1',\n",
      "       'refractive_dioptre_2', 'refractive_astigmatism',\n",
      "       'phakic_or_pseudophakic', 'iop_perkins', 'iop_pneumatic', 'pachymetry',\n",
      "       'axial_length', 'visual_field_mean_defect'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# print the columns\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkQAAAHFCAYAAAAT5Oa6AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAABDFklEQVR4nO3de1xVdb7/8feWm4iwFZSNJN4SPZrXsEGsRg2viVTWWOGQzXhrTI3UdMyZ1E4/TDupTUxmZqKp2TlzpKnsMGKWZYoixXjJHDuZlxQxw40XAsXv748erOMWMDR1o+v1fDzW49H6rs9e67M22wfvvnuthcMYYwQAAGBjtbzdAAAAgLcRiAAAgO0RiAAAgO0RiAAAgO0RiAAAgO0RiAAAgO0RiAAAgO0RiAAAgO0RiAAAgO0RiGBL6enpcjgc1lK7dm1FRESoZ8+emjlzpgoKCiq8Zvr06XI4HJd0nNOnT2v69On6+OOPL+l1lR2rWbNmSkhIuKT9/JwVK1Zo3rx5lW5zOByaPn36FT3elfbhhx+qS5cuCgoKksPh0DvvvHPR+iNHjujpp59Wp06dFBISIn9/fzVu3FiDBg3Su+++q7KyMqv2448/lsPhuOSfnZ316NFD7dq1uyL7Kv83unXr1iuyv/P3+e23316xfeLG4evtBgBvWrx4sf7t3/5NZ86cUUFBgTZs2KBZs2bpP/7jP/T222+rV69eVu3w4cPVr1+/S9r/6dOnNWPGDEk//bKorss51uVYsWKFduzYoZSUlArbNm3apMaNG1/1Hi6XMUaDBw9Wq1at9O677yooKEitW7eusj47O1uJiYkyxugPf/iDunbtqrp162r//v167733NGjQIC1YsEDDhg27hmcBoKYgEMHW2rVrpy5duljr999/v5588kndcccdGjRokPbs2SOXyyVJaty48VUPCKdPn1adOnWuybF+TteuXb16/J9z6NAh/fDDD7rvvvsUHx9/0drjx4/r3nvvVd26dfXZZ5+pUaNGHtt/+9vfatu2bTp27NjVbBlADcZXZsAFmjRpohdffFEnTpzQggULrPHKvsZat26devToobCwMAUGBqpJkya6//77dfr0aX377bdq2LChJGnGjBnW13OPPvqox/4+//xzPfDAA6pfv75uvvnmKo9VLiMjQx06dFDt2rXVokUL/eUvf/HYXtXXAhd+BdSjRw+tXr1a+/bt8/j6sFxlX5nt2LFD99xzj+rXr6/atWurU6dOWrJkSaXHeeuttzR16lRFRkYqJCREvXr10u7du6t+48+zYcMGxcfHKzg4WHXq1FG3bt20evVqa/v06dOtwDh58mQ5HA41a9asyv0tXLhQR44c0ezZsyuEoXIdOnRQz549L9rX1q1b9dBDD6lZs2YKDAxUs2bN9PDDD2vfvn0edVX9/Kr62axYsUJxcXGqW7eu6tatq06dOmnRokUeNW+88YY6duyo2rVrKzQ0VPfdd5927drlUfPoo4+qbt26+uqrr9S3b18FBQWpUaNGev755yX9NEt2xx13KCgoSK1atarwszt69KhGjx6ttm3bqm7dugoPD9ddd92lTz/99KLvy6Wo7ntYrrCwUL/73e8UGhqqoKAgDRw4UN98802FurVr1yo+Pl4hISGqU6eObr/9dn344Yc/288XX3yhhIQEhYeHKyAgQJGRkRowYIAOHjz4i88V1xcCEVCJu+++Wz4+Pvrkk0+qrPn22281YMAA+fv764033lBmZqaef/55BQUFqbS0VI0aNVJmZqYkadiwYdq0aZM2bdqkP//5zx77GTRokFq2bKn/+q//0quvvnrRvvLy8pSSkqInn3xSGRkZ6tatm5544gn9x3/8xyWf4yuvvKLbb79dERERVm+bNm2qsn737t3q1q2bdu7cqb/85S9atWqV2rZtq0cffVSzZ8+uUP/0009r3759ev311/Xaa69pz549GjhwoMd1OpVZv3697rrrLrndbi1atEhvvfWWgoODNXDgQL399tuSfvpKcdWqVZKksWPHatOmTcrIyKhyn1lZWfLx8dHdd99dnbemSt9++61at26tefPm6R//+IdmzZqlw4cP67bbbtP3339/Wft85plnNGTIEEVGRio9PV0ZGRkaOnSoR0CYOXOmhg0bpltuuUWrVq3SSy+9pG3btikuLk579uzx2N+ZM2c0aNAgDRgwQH//+9/Vv39/TZkyRU8//bSGDh2q3//+98rIyFDr1q316KOPKjc313rtDz/8IEmaNm2aVq9ercWLF6tFixbq0aPHFbuW6lLfw2HDhqlWrVrW9W5btmxRjx49dPz4catm2bJl6tOnj0JCQrRkyRL953/+p0JDQ9W3b9+LhqJTp06pd+/eOnLkiP76178qKytL8+bNU5MmTXTixIkrcr64jhjAhhYvXmwkmZycnCprXC6XadOmjbU+bdo0c/4/mb/97W9GksnLy6tyH0ePHjWSzLRp0ypsK9/fM888U+W28zVt2tQ4HI4Kx+vdu7cJCQkxp06d8ji3vXv3etR99NFHRpL56KOPrLEBAwaYpk2bVtr7hX0/9NBDJiAgwOzfv9+jrn///qZOnTrm+PHjHse5++67Per+8z//00gymzZtqvR45bp27WrCw8PNiRMnrLGzZ8+adu3amcaNG5tz584ZY4zZu3evkWReeOGFi+7PGGP+7d/+zURERFQYLysrM2fOnLGWsrIya1tl79eFzp49a06ePGmCgoLMSy+9ZI1X9vMzpuLP5ptvvjE+Pj5myJAhVR6jsLDQBAYGVng/9+/fbwICAkxSUpI1NnToUCPJ/Pd//7c1dubMGdOwYUMjyXz++efW+LFjx4yPj48ZP378Rc/vzJkzJj4+3tx3331V1pXr3r27ueWWW3627sJjVPYelr9XFx73s88+M5LMc889Z4wx5tSpUyY0NNQMHDjQo66srMx07NjR/OpXv6qwz/L3f+vWrUaSeeeddy6pZ9yYmCECqmCMuej2Tp06yd/fXyNHjtSSJUsqncavjvvvv7/atbfccos6duzoMZaUlKSioiJ9/vnnl3X86lq3bp3i4+MVFRXlMf7oo4/q9OnTFWaXEhMTPdY7dOggSVV+NSL99H/smzdv1gMPPKC6deta4z4+PkpOTtbBgwer/bVbdYwfP15+fn7WcmHPFzp58qQmT56sli1bytfXV76+vqpbt65OnTpV4eur6sjKylJZWZkef/zxKms2bdqk4uJi66vWclFRUbrrrrsqzIA4HA6PmTBfX1+1bNlSjRo1UufOna3x0NBQhYeHV/h5vPrqq7r11ltVu3Zt+fr6ys/PTx9++OFlnV9lLvU9HDJkiMd6t27d1LRpU3300UeSpI0bN+qHH37Q0KFDdfbsWWs5d+6c+vXrp5ycHJ06darSXlq2bKn69etr8uTJevXVV/Xll19ekXPE9YlABFTi1KlTOnbsmCIjI6usufnmm7V27VqFh4fr8ccf180336ybb75ZL7300iUdq6prWioTERFR5djVviD42LFjlfZa/h5dePywsDCP9YCAAElScXFxlccoLCyUMeaSjlMdTZo00dGjR3X69GmP8QkTJignJ0c5OTnV+jkkJSUpLS1Nw4cP1z/+8Q9t2bJFOTk5atiw4UXPqypHjx6VpIteQF9+vlW9Jxe+H3Xq1FHt2rU9xvz9/RUaGlrh9f7+/vrxxx+t9Tlz5ugPf/iDYmNj9d///d/Kzs5WTk6O+vXrd1nnV5lLfQ+r+syXn/eRI0ckSQ888IBHuPXz89OsWbNkjLG+CryQ0+nU+vXr1alTJz399NO65ZZbFBkZqWnTpunMmTNX5Hxx/eAuM6ASq1evVllZ2c/eKn/nnXfqzjvvVFlZmbZu3aqXX35ZKSkpcrlceuihh6p1rEt5tlF+fn6VY+UBpPyXYUlJiUfd5V7jUi4sLEyHDx+uMH7o0CFJUoMGDX7R/iWpfv36qlWr1hU/Tu/evbVmzRp98MEHeuCBB6zxqKgoa8bL39//ovtwu916//33NW3aNP3xj3+0xktKSir8wj3/Z1AeBKWKP4Pyi+4PHjxYYeatXPnPtar35Eq87+WWLVumHj16aP78+R7jV+p6mkt5D8tV9Zlv2bKlpP/7PLz88stV3hlZfqdoZdq3b6+VK1fKGKNt27YpPT1dzz77rAIDAz16xI2PGSLgAvv379fEiRPldDo1atSoar3Gx8dHsbGx+utf/ypJ1tdX1ZkVuRQ7d+7UP//5T4+xFStWKDg4WLfeeqskWXdbbdu2zaPu3XffrbC/gICAavcWHx+vdevWWcGk3NKlS1WnTp0rcpt+UFCQYmNjtWrVKo++zp07p2XLlqlx48Zq1arVJe93+PDhcrlcmjRpUqXBojocDoeMMR4BR5Jef/31CheKV/UzeO+99zzW+/TpIx8fnwoB5HxxcXEKDAzUsmXLPMYPHjxofY15pTgcjgrnt23btotebH+p+6/ue1hu+fLlHusbN27Uvn37rP9Zuf3221WvXj19+eWX6tKlS6XLz4Xd8t46duyouXPnql69elf9K2jUPMwQwdZ27NhhXXNQUFCgTz/9VIsXL5aPj48yMjKs/4OvzKuvvqp169ZpwIABatKkiX788Ue98cYbkmQ90DE4OFhNmzbV3//+d8XHxys0NFQNGjS46C3iFxMZGanExERNnz5djRo10rJly5SVlaVZs2apTp06kqTbbrtNrVu31sSJE3X27FnVr19fGRkZ2rBhQ4X9tW/fXqtWrdL8+fMVExOjWrVqeTyX6XzTpk3T+++/r549e+qZZ55RaGioli9frtWrV2v27NlyOp2XdU4Xmjlzpnr37q2ePXtq4sSJ8vf31yuvvKIdO3borbfeuuSnhUtSvXr19M4772jgwIHq2LGjx4MZjx07pk8++UT5+fnq1q1blfsICQnRr3/9a73wwgvWz3D9+vVatGiR6tWr51F79913KzQ0VMOGDdOzzz4rX19fpaen68CBAx51zZo109NPP61///d/V3FxsR5++GE5nU59+eWX+v777zVjxgzVq1dPf/7zn/X000/rkUce0cMPP6xjx45pxowZql27tqZNm3bJ70dVEhIS9O///u+aNm2aunfvrt27d+vZZ59V8+bNdfbs2Wrto6ioSH/7298qjDds2FDdu3ev9ntYbuvWrRo+fLh+85vf6MCBA5o6dapuuukmjR49WpJUt25dvfzyyxo6dKh++OEHPfDAAwoPD9fRo0f1z3/+U0ePHq0ycL7//vt65ZVXdO+996pFixYyxmjVqlU6fvy4evfuXb03DTcOL17QDXhN+d0m5Yu/v78JDw833bt3N6mpqaagoKDCay68c2jTpk3mvvvuM02bNjUBAQEmLCzMdO/e3bz77rser1u7dq3p3LmzCQgIMJLM0KFDPfZ39OjRnz2WMT/dZTZgwADzt7/9zdxyyy3G39/fNGvWzMyZM6fC6//1r3+ZPn36mJCQENOwYUMzduxYs3r16gp3Tf3www/mgQceMPXq1TMOh8PjmKrk7rjt27ebgQMHGqfTafz9/U3Hjh3N4sWLPWrK7876r//6L4/x8rvCLqyvzKeffmruuusuExQUZAIDA03Xrl3Ne++9V+n+qnOXWbn8/HwzZcoU06FDBxMUFGT8/PxMZGSkGThwoFm6dKk5c+ZMhfM4//06ePCguf/++039+vVNcHCw6devn9mxY4dp2rSp9XMtt2XLFtOtWzcTFBRkbrrpJjNt2jTz+uuvV3oH4NKlS81tt91mateuberWrWs6d+5c4X16/fXXTYcOHYy/v79xOp3mnnvuMTt37vSoGTp0qAkKCqpw3lXd/VX+mSpXUlJiJk6caG666SZTu3Ztc+utt5p33nnHDB06tMq7ES88zvn/rs5funfvfknvYfm/0TVr1pjk5GRTr1496267PXv2VDj2+vXrzYABA0xoaKjx8/MzN910kxkwYIDH5/DCu8y++uor8/DDD5ubb77ZBAYGGqfTaX71q1+Z9PT0nz1X3HgcxvzMrTQAAAA3OK4hAgAAtkcgAgAAtkcgAgAAtkcgAgAAtkcgAgAAtkcgAgAAtseDGavp3LlzOnTokIKDgy/rwXAAAODaM8boxIkTioyMVK1aVc8DEYiq6dChQ1X+rSEAAFCzHThw4KJ/SJlAVE3BwcGSfnpDQ0JCvNwNAACojqKiIkVFRVm/x6tCIKqm8q/JQkJCCEQAAFxnfu5yFy6qBgAAtkcgAgAAtkcgAgAAtkcgAgAAtkcgAgAAtkcgAgAAtkcgAgAAtkcgAgAAtkcgAgAAtkcgAgAAtkcgAgAAtkcgAgAAtkcgAgAAtkcgAgAAtkcgAgAAtufr7QYAeFfMU0u93QJqkNwXHvF2C4BXMEMEAABsj0AEAABsj0AEAABsj0AEAABsj0AEAABsj0AEAABsj0AEAABsj0AEAABsj0AEAABsj0AEAABsj0AEAABsj0AEAABsj0AEAABsj0AEAABsj0AEAABsj0AEAABsj0AEAABsj0AEAABsj0AEAABsj0AEAABsz6uBqFmzZnI4HBWWxx9/XJJkjNH06dMVGRmpwMBA9ejRQzt37vTYR0lJicaOHasGDRooKChIiYmJOnjwoEdNYWGhkpOT5XQ65XQ6lZycrOPHj1+r0wQAADWcVwNRTk6ODh8+bC1ZWVmSpN/85jeSpNmzZ2vOnDlKS0tTTk6OIiIi1Lt3b504ccLaR0pKijIyMrRy5Upt2LBBJ0+eVEJCgsrKyqyapKQk5eXlKTMzU5mZmcrLy1NycvK1PVkAAFBjOYwxxttNlEtJSdH777+vPXv2SJIiIyOVkpKiyZMnS/ppNsjlcmnWrFkaNWqU3G63GjZsqDfffFMPPvigJOnQoUOKiorSBx98oL59+2rXrl1q27atsrOzFRsbK0nKzs5WXFycvvrqK7Vu3bpavRUVFcnpdMrtdiskJOQqnD3gHTFPLfV2C6hBcl94xNstAFdUdX9/15hriEpLS7Vs2TL9/ve/l8Ph0N69e5Wfn68+ffpYNQEBAerevbs2btwoScrNzdWZM2c8aiIjI9WuXTurZtOmTXI6nVYYkqSuXbvK6XRaNZUpKSlRUVGRxwIAAG5MNSYQvfPOOzp+/LgeffRRSVJ+fr4kyeVyedS5XC5rW35+vvz9/VW/fv2L1oSHh1c4Xnh4uFVTmZkzZ1rXHDmdTkVFRV32uQEAgJqtxgSiRYsWqX///oqMjPQYdzgcHuvGmApjF7qwprL6n9vPlClT5Ha7reXAgQPVOQ0AAHAdqhGBaN++fVq7dq2GDx9ujUVEREhShVmcgoICa9YoIiJCpaWlKiwsvGjNkSNHKhzz6NGjFWafzhcQEKCQkBCPBQAA3JhqRCBavHixwsPDNWDAAGusefPmioiIsO48k366zmj9+vXq1q2bJCkmJkZ+fn4eNYcPH9aOHTusmri4OLndbm3ZssWq2bx5s9xut1UDAADszdfbDZw7d06LFy/W0KFD5ev7f+04HA6lpKQoNTVV0dHRio6OVmpqqurUqaOkpCRJktPp1LBhwzRhwgSFhYUpNDRUEydOVPv27dWrVy9JUps2bdSvXz+NGDFCCxYskCSNHDlSCQkJ1b7DDAAA3Ni8HojWrl2r/fv36/e//32FbZMmTVJxcbFGjx6twsJCxcbGas2aNQoODrZq5s6dK19fXw0ePFjFxcWKj49Xenq6fHx8rJrly5dr3Lhx1t1oiYmJSktLu/onBwAArgs16jlENRnPIcKNiucQ4Xw8hwg3muvuOUQAAADeQiACAAC2RyACAAC2RyACAAC2RyACAAC2RyACAAC2RyACAAC2RyACAAC2RyACAAC2RyACAAC2RyACAAC2RyACAAC2RyACAAC2RyACAAC2RyACAAC2RyACAAC2RyACAAC2RyACAAC2RyACAAC2RyACAAC2RyACAAC2RyACAAC2RyACAAC2RyACAAC2RyACAAC2RyACAAC2RyACAAC2RyACAAC2RyACAAC2RyACAAC2RyACAAC2RyACAAC2RyACAAC2RyACAAC2RyACAAC2RyACAAC25/VA9N133+m3v/2twsLCVKdOHXXq1Em5ubnWdmOMpk+frsjISAUGBqpHjx7auXOnxz5KSko0duxYNWjQQEFBQUpMTNTBgwc9agoLC5WcnCyn0ymn06nk5GQdP378WpwiAACo4bwaiAoLC3X77bfLz89P//M//6Mvv/xSL774ourVq2fVzJ49W3PmzFFaWppycnIUERGh3r1768SJE1ZNSkqKMjIytHLlSm3YsEEnT55UQkKCysrKrJqkpCTl5eUpMzNTmZmZysvLU3Jy8rU8XQAAUEM5jDHGWwf/4x//qM8++0yffvpppduNMYqMjFRKSoomT54s6afZIJfLpVmzZmnUqFFyu91q2LCh3nzzTT344IOSpEOHDikqKkoffPCB+vbtq127dqlt27bKzs5WbGysJCk7O1txcXH66quv1Lp165/ttaioSE6nU263WyEhIVfoHQC8L+appd5uATVI7guPeLsF4Iqq7u9vr84Qvfvuu+rSpYt+85vfKDw8XJ07d9bChQut7Xv37lV+fr769OljjQUEBKh79+7auHGjJCk3N1dnzpzxqImMjFS7du2smk2bNsnpdFphSJK6du0qp9Np1VyopKRERUVFHgsAALgxeTUQffPNN5o/f76io6P1j3/8Q4899pjGjRunpUt/+j/W/Px8SZLL5fJ4ncvlsrbl5+fL399f9evXv2hNeHh4heOHh4dbNReaOXOmdb2R0+lUVFTULztZAABQY3k1EJ07d0633nqrUlNT1blzZ40aNUojRozQ/PnzPeocDofHujGmwtiFLqyprP5i+5kyZYrcbre1HDhwoLqnBQAArjNeDUSNGjVS27ZtPcbatGmj/fv3S5IiIiIkqcIsTkFBgTVrFBERodLSUhUWFl605siRIxWOf/To0QqzT+UCAgIUEhLisQAAgBuTVwPR7bffrt27d3uM/etf/1LTpk0lSc2bN1dERISysrKs7aWlpVq/fr26desmSYqJiZGfn59HzeHDh7Vjxw6rJi4uTm63W1u2bLFqNm/eLLfbbdUAAAD78vXmwZ988kl169ZNqampGjx4sLZs2aLXXntNr732mqSfvuZKSUlRamqqoqOjFR0drdTUVNWpU0dJSUmSJKfTqWHDhmnChAkKCwtTaGioJk6cqPbt26tXr16Sfpp16tevn0aMGKEFCxZIkkaOHKmEhIRq3WEGAABubF4NRLfddpsyMjI0ZcoUPfvss2revLnmzZunIUOGWDWTJk1ScXGxRo8ercLCQsXGxmrNmjUKDg62aubOnStfX18NHjxYxcXFio+PV3p6unx8fKya5cuXa9y4cdbdaImJiUpLS7t2JwsAAGosrz6H6HrCc4hwo+I5RDgfzyHCjea6eA4RAABATUAgAgAAtkcgAgAAtkcgAgAAtkcgAgAAtkcgAgAAtkcgAgAAtkcgAgAAtkcgAgAAtkcgAgAAtkcgAgAAtkcgAgAAtkcgAgAAtkcgAgAAtkcgAgAAtkcgAgAAtkcgAgAAtkcgAgAAtkcgAgAAtkcgAgAAtkcgAgAAtkcgAgAAtkcgAgAAtkcgAgAAtkcgAgAAtkcgAgAAtkcgAgAAtkcgAgAAtkcgAgAAtkcgAgAAtkcgAgAAtkcgAgAAtkcgAgAAtkcgAgAAtkcgAgAAtufVQDR9+nQ5HA6PJSIiwtpujNH06dMVGRmpwMBA9ejRQzt37vTYR0lJicaOHasGDRooKChIiYmJOnjwoEdNYWGhkpOT5XQ65XQ6lZycrOPHj1+LUwQAANcBr88Q3XLLLTp8+LC1bN++3do2e/ZszZkzR2lpacrJyVFERIR69+6tEydOWDUpKSnKyMjQypUrtWHDBp08eVIJCQkqKyuzapKSkpSXl6fMzExlZmYqLy9PycnJ1/Q8AQBAzeXr9QZ8fT1mhcoZYzRv3jxNnTpVgwYNkiQtWbJELpdLK1as0KhRo+R2u7Vo0SK9+eab6tWrlyRp2bJlioqK0tq1a9W3b1/t2rVLmZmZys7OVmxsrCRp4cKFiouL0+7du9W6detrd7IAAKBG8voM0Z49exQZGanmzZvroYce0jfffCNJ2rt3r/Lz89WnTx+rNiAgQN27d9fGjRslSbm5uTpz5oxHTWRkpNq1a2fVbNq0SU6n0wpDktS1a1c5nU6rBgAA2JtXZ4hiY2O1dOlStWrVSkeOHNFzzz2nbt26aefOncrPz5ckuVwuj9e4XC7t27dPkpSfny9/f3/Vr1+/Qk356/Pz8xUeHl7h2OHh4VZNZUpKSlRSUmKtFxUVXd5JAgCAGs+rgah///7Wf7dv315xcXG6+eabtWTJEnXt2lWS5HA4PF5jjKkwdqELayqr/7n9zJw5UzNmzKjWeQAAgOub178yO19QUJDat2+vPXv2WNcVXTiLU1BQYM0aRUREqLS0VIWFhRetOXLkSIVjHT16tMLs0/mmTJkit9ttLQcOHPhF5wYAAGquGhWISkpKtGvXLjVq1EjNmzdXRESEsrKyrO2lpaVav369unXrJkmKiYmRn5+fR83hw4e1Y8cOqyYuLk5ut1tbtmyxajZv3iy3223VVCYgIEAhISEeCwAAuDF59SuziRMnauDAgWrSpIkKCgr03HPPqaioSEOHDpXD4VBKSopSU1MVHR2t6Ohopaamqk6dOkpKSpIkOZ1ODRs2TBMmTFBYWJhCQ0M1ceJEtW/f3rrrrE2bNurXr59GjBihBQsWSJJGjhyphIQE7jADAACSvByIDh48qIcffljff/+9GjZsqK5duyo7O1tNmzaVJE2aNEnFxcUaPXq0CgsLFRsbqzVr1ig4ONjax9y5c+Xr66vBgweruLhY8fHxSk9Pl4+Pj1WzfPlyjRs3zrobLTExUWlpadf2ZAEAQI3lMMYYbzdxPSgqKpLT6ZTb7ebrM9xQYp5a6u0WUIPkvvCIt1sArqjq/v6uUdcQAQAAeAOBCAAA2B6BCAAA2B6BCAAA2J7X/7grAADn40J/nO9aXejPDBEAALA9AhEAALA9AhEAALA9AhEAALA9AhEAALA9AhEAALA9AhEAALA9AhEAALA9AhEAALA9AhEAALA9AhEAALA9AhEAALA9AhEAALA9AhEAALA9AhEAALA9AhEAALA9AhEAALA9AhEAALA9AhEAALC9ywpELVq00LFjxyqMHz9+XC1atPjFTQEAAFxLlxWIvv32W5WVlVUYLykp0XffffeLmwIAALiWfC+l+N1337X++x//+IecTqe1XlZWpg8//FDNmjW7Ys0BAABcC5cUiO69915JksPh0NChQz22+fn5qVmzZnrxxRevWHMAAADXwiUFonPnzkmSmjdvrpycHDVo0OCqNAUAAHAtXVIgKrd3794r3QcAAIDXXFYgkqQPP/xQH374oQoKCqyZo3JvvPHGL24MAADgWrmsQDRjxgw9++yz6tKlixo1aiSHw3Gl+wIAALhmLisQvfrqq0pPT1dycvKV7gcAAOCau6znEJWWlqpbt25XuhcAAACvuKxANHz4cK1YseJK9wIAAOAVl/WV2Y8//qjXXntNa9euVYcOHeTn5+exfc6cOVekOQAAgGvhsmaItm3bpk6dOqlWrVrasWOHvvjiC2vJy8u7rEZmzpwph8OhlJQUa8wYo+nTpysyMlKBgYHq0aOHdu7c6fG6kpISjR07Vg0aNFBQUJASExN18OBBj5rCwkIlJyfL6XTK6XQqOTlZx48fv6w+AQDAjeeyZog++uijK9pETk6OXnvtNXXo0MFjfPbs2ZozZ47S09PVqlUrPffcc+rdu7d2796t4OBgSVJKSoree+89rVy5UmFhYZowYYISEhKUm5srHx8fSVJSUpIOHjyozMxMSdLIkSOVnJys995774qeBwAAuD5d1gzRlXTy5EkNGTJECxcuVP369a1xY4zmzZunqVOnatCgQWrXrp2WLFmi06dPW9cvud1uLVq0SC+++KJ69eqlzp07a9myZdq+fbvWrl0rSdq1a5cyMzP1+uuvKy4uTnFxcVq4cKHef/997d692yvnDAAAapbLmiHq2bPnRZ89tG7dumrv6/HHH9eAAQPUq1cvPffcc9b43r17lZ+frz59+lhjAQEB6t69uzZu3KhRo0YpNzdXZ86c8aiJjIxUu3bttHHjRvXt21ebNm2S0+lUbGysVdO1a1c5nU5t3LhRrVu3rrSvkpISlZSUWOtFRUXVPicAAHB9uaxA1KlTJ4/1M2fOKC8vTzt27KjwR18vZuXKlfr888+Vk5NTYVt+fr4kyeVyeYy7XC7t27fPqvH39/eYWSqvKX99fn6+wsPDK+w/PDzcqqnMzJkzNWPGjGqfCwAAuH5dViCaO3dupePTp0/XyZMnq7WPAwcO6IknntCaNWtUu3btKusunIkyxvzsk7EvrKms/uf2M2XKFI0fP95aLyoqUlRU1EWPCwAArk9X9Bqi3/72t9X+O2a5ubkqKChQTEyMfH195evrq/Xr1+svf/mLfH19rZmhC2dxCgoKrG0REREqLS1VYWHhRWuOHDlS4fhHjx6tMPt0voCAAIWEhHgsAADgxnRFA9GmTZsuOttzvvj4eG3fvl15eXnW0qVLFw0ZMkR5eXlq0aKFIiIilJWVZb2mtLRU69evt56SHRMTIz8/P4+aw4cPa8eOHVZNXFyc3G63tmzZYtVs3rxZbrebp20DAABJl/mV2aBBgzzWjTE6fPiwtm7dqj//+c/V2kdwcLDatWvnMRYUFKSwsDBrPCUlRampqYqOjlZ0dLRSU1NVp04dJSUlSZKcTqeGDRumCRMmKCwsTKGhoZo4caLat2+vXr16SZLatGmjfv36acSIEVqwYIGkn267T0hIqPKCagAAYC+XFYicTqfHeq1atdS6dWs9++yzHnd8/VKTJk1ScXGxRo8ercLCQsXGxmrNmjXWM4ikn65n8vX11eDBg1VcXKz4+Hilp6dbzyCSpOXLl2vcuHFWb4mJiUpLS7tifQIAgOubwxhjvN3E9aCoqEhOp1Nut5vriXBDiXlqqbdbQA2S+8Ij3m6BzyQ8/NLPZHV/f1/WDFG53Nxc7dq1Sw6HQ23btlXnzp1/ye4AAAC84rICUUFBgR566CF9/PHHqlevnowxcrvd6tmzp1auXKmGDRte6T4BAACumsu6y2zs2LEqKirSzp079cMPP6iwsFA7duxQUVGRxo0bd6V7BAAAuKoua4YoMzNTa9euVZs2bayxtm3b6q9//esVvagaAADgWrisGaJz587Jz8+vwrifn5/OnTv3i5sCAAC4li4rEN1111164okndOjQIWvsu+++05NPPqn4+Pgr1hwAAMC1cFmBKC0tTSdOnFCzZs108803q2XLlmrevLlOnDihl19++Ur3CAAAcFVd1jVEUVFR+vzzz5WVlaWvvvpKxhi1bdvWejo0AADA9eSSZojWrVuntm3bqqioSJLUu3dvjR07VuPGjdNtt92mW265RZ9++ulVaRQAAOBquaRANG/ePI0YMaLSJz06nU6NGjVKc+bMuWLNAQAAXAuXFIj++c9/ql+/flVu79Onj3Jzc39xUwAAANfSJQWiI0eOVHq7fTlfX18dPXr0FzcFAABwLV1SILrpppu0ffv2Krdv27ZNjRo1+sVNAQAAXEuXFIjuvvtuPfPMM/rxxx8rbCsuLta0adOUkJBwxZoDAAC4Fi7ptvs//elPWrVqlVq1aqUxY8aodevWcjgc2rVrl/7617+qrKxMU6dOvVq9AgAAXBWXFIhcLpc2btyoP/zhD5oyZYqMMZIkh8Ohvn376pVXXpHL5boqjQIAAFwtl/xgxqZNm+qDDz5QYWGhvv76axljFB0drfr161+N/gAAAK66y3pStSTVr19ft91225XsBQAAwCsu62+ZAQAA3EgIRAAAwPYIRAAAwPYIRAAAwPYIRAAAwPYIRAAAwPYIRAAAwPYIRAAAwPYIRAAAwPYIRAAAwPYIRAAAwPYIRAAAwPYIRAAAwPYIRAAAwPYIRAAAwPYIRAAAwPYIRAAAwPYIRAAAwPa8Gojmz5+vDh06KCQkRCEhIYqLi9P//M//WNuNMZo+fboiIyMVGBioHj16aOfOnR77KCkp0dixY9WgQQMFBQUpMTFRBw8e9KgpLCxUcnKynE6nnE6nkpOTdfz48WtxigAA4Drg1UDUuHFjPf/889q6dau2bt2qu+66S/fcc48VembPnq05c+YoLS1NOTk5ioiIUO/evXXixAlrHykpKcrIyNDKlSu1YcMGnTx5UgkJCSorK7NqkpKSlJeXp8zMTGVmZiovL0/JycnX/HwBAEDN5DDGGG83cb7Q0FC98MIL+v3vf6/IyEilpKRo8uTJkn6aDXK5XJo1a5ZGjRolt9uthg0b6s0339SDDz4oSTp06JCioqL0wQcfqG/fvtq1a5fatm2r7OxsxcbGSpKys7MVFxenr776Sq1bt65WX0VFRXI6nXK73QoJCbk6Jw94QcxTS73dAmqQ3Bce8XYLfCbh4Zd+Jqv7+7vGXENUVlamlStX6tSpU4qLi9PevXuVn5+vPn36WDUBAQHq3r27Nm7cKEnKzc3VmTNnPGoiIyPVrl07q2bTpk1yOp1WGJKkrl27yul0WjWVKSkpUVFRkccCAABuTF4PRNu3b1fdunUVEBCgxx57TBkZGWrbtq3y8/MlSS6Xy6Pe5XJZ2/Lz8+Xv76/69etftCY8PLzCccPDw62aysycOdO65sjpdCoqKuoXnScAAKi5vB6IWrdurby8PGVnZ+sPf/iDhg4dqi+//NLa7nA4POqNMRXGLnRhTWX1P7efKVOmyO12W8uBAweqe0oAAOA64/VA5O/vr5YtW6pLly6aOXOmOnbsqJdeekkRERGSVGEWp6CgwJo1ioiIUGlpqQoLCy9ac+TIkQrHPXr0aIXZp/MFBARYd7+VLwAA4Mbk9UB0IWOMSkpK1Lx5c0VERCgrK8vaVlpaqvXr16tbt26SpJiYGPn5+XnUHD58WDt27LBq4uLi5Ha7tWXLFqtm8+bNcrvdVg0AALA3X28e/Omnn1b//v0VFRWlEydOaOXKlfr444+VmZkph8OhlJQUpaamKjo6WtHR0UpNTVWdOnWUlJQkSXI6nRo2bJgmTJigsLAwhYaGauLEiWrfvr169eolSWrTpo369eunESNGaMGCBZKkkSNHKiEhodp3mAEAgBubVwPRkSNHlJycrMOHD8vpdKpDhw7KzMxU7969JUmTJk1ScXGxRo8ercLCQsXGxmrNmjUKDg629jF37lz5+vpq8ODBKi4uVnx8vNLT0+Xj42PVLF++XOPGjbPuRktMTFRaWtq1PVkAAFBj1bjnENVUPIcINyqe+YLz8Rwi1DS2ew4RAACAtxCIAACA7RGIAACA7RGIAACA7RGIAACA7RGIAACA7RGIAACA7RGIAACA7RGIAACA7RGIAACA7RGIAACA7RGIAACA7RGIAACA7RGIAACA7RGIAACA7RGIAACA7RGIAACA7RGIAACA7RGIAACA7RGIAACA7RGIAACA7RGIAACA7RGIAACA7RGIAACA7RGIAACA7RGIAACA7RGIAACA7RGIAACA7RGIAACA7RGIAACA7RGIAACA7RGIAACA7RGIAACA7RGIAACA7RGIAACA7Xk1EM2cOVO33XabgoODFR4ernvvvVe7d+/2qDHGaPr06YqMjFRgYKB69OihnTt3etSUlJRo7NixatCggYKCgpSYmKiDBw961BQWFio5OVlOp1NOp1PJyck6fvz41T5FAABwHfBqIFq/fr0ef/xxZWdnKysrS2fPnlWfPn106tQpq2b27NmaM2eO0tLSlJOTo4iICPXu3VsnTpywalJSUpSRkaGVK1dqw4YNOnnypBISElRWVmbVJCUlKS8vT5mZmcrMzFReXp6Sk5Ov6fkCAICaydebB8/MzPRYX7x4scLDw5Wbm6tf//rXMsZo3rx5mjp1qgYNGiRJWrJkiVwul1asWKFRo0bJ7XZr0aJFevPNN9WrVy9J0rJlyxQVFaW1a9eqb9++2rVrlzIzM5Wdna3Y2FhJ0sKFCxUXF6fdu3erdevW1/bEAQBAjVKjriFyu92SpNDQUEnS3r17lZ+frz59+lg1AQEB6t69uzZu3ChJys3N1ZkzZzxqIiMj1a5dO6tm06ZNcjqdVhiSpK5du8rpdFo1FyopKVFRUZHHAgAAbkw1JhAZYzR+/HjdcccdateunSQpPz9fkuRyuTxqXS6XtS0/P1/+/v6qX7/+RWvCw8MrHDM8PNyqudDMmTOt642cTqeioqJ+2QkCAIAaq8YEojFjxmjbtm166623KmxzOBwe68aYCmMXurCmsvqL7WfKlClyu93WcuDAgeqcBgAAuA7ViEA0duxYvfvuu/roo4/UuHFjazwiIkKSKsziFBQUWLNGERERKi0tVWFh4UVrjhw5UuG4R48erTD7VC4gIEAhISEeCwAAuDF5NRAZYzRmzBitWrVK69atU/PmzT22N2/eXBEREcrKyrLGSktLtX79enXr1k2SFBMTIz8/P4+aw4cPa8eOHVZNXFyc3G63tmzZYtVs3rxZbrfbqgEAAPbl1bvMHn/8ca1YsUJ///vfFRwcbM0EOZ1OBQYGyuFwKCUlRampqYqOjlZ0dLRSU1NVp04dJSUlWbXDhg3ThAkTFBYWptDQUE2cOFHt27e37jpr06aN+vXrpxEjRmjBggWSpJEjRyohIYE7zAAAgHcD0fz58yVJPXr08BhfvHixHn30UUnSpEmTVFxcrNGjR6uwsFCxsbFas2aNgoODrfq5c+fK19dXgwcPVnFxseLj45Weni4fHx+rZvny5Ro3bpx1N1piYqLS0tKu7gkCAIDrgsMYY7zdxPWgqKhITqdTbreb64lwQ4l5aqm3W0ANkvvCI95ugc8kPPzSz2R1f3/XiIuqAQAAvIlABAAAbI9ABAAAbI9ABAAAbI9ABAAAbI9ABAAAbI9ABAAAbI9ABAAAbI9ABAAAbI9ABAAAbI9ABAAAbI9ABAAAbI9ABAAAbI9ABAAAbI9ABAAAbI9ABAAAbI9ABAAAbI9ABAAAbI9ABAAAbI9ABAAAbI9ABAAAbI9ABAAAbI9ABAAAbI9ABAAAbI9ABAAAbI9ABAAAbI9ABAAAbI9ABAAAbI9ABAAAbI9ABAAAbI9ABAAAbI9ABAAAbI9ABAAAbI9ABAAAbI9ABAAAbI9ABAAAbM+rgeiTTz7RwIEDFRkZKYfDoXfeecdjuzFG06dPV2RkpAIDA9WjRw/t3LnTo6akpERjx45VgwYNFBQUpMTERB08eNCjprCwUMnJyXI6nXI6nUpOTtbx48ev8tkBAIDrhVcD0alTp9SxY0elpaVVun327NmaM2eO0tLSlJOTo4iICPXu3VsnTpywalJSUpSRkaGVK1dqw4YNOnnypBISElRWVmbVJCUlKS8vT5mZmcrMzFReXp6Sk5Ov+vkBAIDrg683D96/f3/179+/0m3GGM2bN09Tp07VoEGDJElLliyRy+XSihUrNGrUKLndbi1atEhvvvmmevXqJUlatmyZoqKitHbtWvXt21e7du1SZmamsrOzFRsbK0lauHCh4uLitHv3brVu3franCwAAKixauw1RHv37lV+fr769OljjQUEBKh79+7auHGjJCk3N1dnzpzxqImMjFS7du2smk2bNsnpdFphSJK6du0qp9Np1VSmpKRERUVFHgsAALgx1dhAlJ+fL0lyuVwe4y6Xy9qWn58vf39/1a9f/6I14eHhFfYfHh5u1VRm5syZ1jVHTqdTUVFRv+h8AABAzVVjA1E5h8PhsW6MqTB2oQtrKqv/uf1MmTJFbrfbWg4cOHCJnQMAgOtFjQ1EERERklRhFqegoMCaNYqIiFBpaakKCwsvWnPkyJEK+z969GiF2afzBQQEKCQkxGMBAAA3phobiJo3b66IiAhlZWVZY6WlpVq/fr26desmSYqJiZGfn59HzeHDh7Vjxw6rJi4uTm63W1u2bLFqNm/eLLfbbdUAAAB78+pdZidPntTXX39tre/du1d5eXkKDQ1VkyZNlJKSotTUVEVHRys6OlqpqamqU6eOkpKSJElOp1PDhg3ThAkTFBYWptDQUE2cOFHt27e37jpr06aN+vXrpxEjRmjBggWSpJEjRyohIYE7zAAAgCQvB6KtW7eqZ8+e1vr48eMlSUOHDlV6eromTZqk4uJijR49WoWFhYqNjdWaNWsUHBxsvWbu3Lny9fXV4MGDVVxcrPj4eKWnp8vHx8eqWb58ucaNG2fdjZaYmFjls48AAID9OIwxxttNXA+KiorkdDrldru5ngg3lJinlnq7BdQguS884u0W+EzCwy/9TFb393eNvYYIAADgWiEQAQAA2yMQAQAA2yMQAQAA2yMQAQAA2yMQAQAA2yMQAQAA2yMQAQAA2yMQAQAA2yMQAQAA2yMQAQAA2yMQAQAA2yMQAQAA2yMQAQAA2yMQAQAA2yMQAQAA2yMQAQAA2yMQAQAA2yMQAQAA2yMQAQAA2yMQAQAA2yMQAQAA2yMQAQAA2yMQAQAA2yMQAQAA2yMQAQAA2yMQAQAA2yMQAQAA2yMQAQAA2yMQAQAA2yMQAQAA2yMQAQAA2yMQAQAA2yMQAQAA2/P1dgN2EvPUUm+3gBom94VHvN0CAEA2myF65ZVX1Lx5c9WuXVsxMTH69NNPvd0SAACoAWwTiN5++22lpKRo6tSp+uKLL3TnnXeqf//+2r9/v7dbAwAAXmabQDRnzhwNGzZMw4cPV5s2bTRv3jxFRUVp/vz53m4NAAB4mS0CUWlpqXJzc9WnTx+P8T59+mjjxo1e6goAANQUtrio+vvvv1dZWZlcLpfHuMvlUn5+fqWvKSkpUUlJibXudrslSUVFRZfdR1lJ8WW/FjemX/J5ulL4XOJ8fCZR0/zSz2T5640xF62zRSAq53A4PNaNMRXGys2cOVMzZsyoMB4VFXVVeoM9OV9+zNstAB74TKKmuVKfyRMnTsjpdFa53RaBqEGDBvLx8akwG1RQUFBh1qjclClTNH78eGv93Llz+uGHHxQWFlZliMLPKyoqUlRUlA4cOKCQkBBvtwNI4nOJmofP5JVjjNGJEycUGRl50TpbBCJ/f3/FxMQoKytL9913nzWelZWle+65p9LXBAQEKCAgwGOsXr16V7NNWwkJCeEfOWocPpeoafhMXhkXmxkqZ4tAJEnjx49XcnKyunTpori4OL322mvav3+/HnuM6WEAAOzONoHowQcf1LFjx/Tss8/q8OHDateunT744AM1bdrU260BAAAvs00gkqTRo0dr9OjR3m7D1gICAjRt2rQKX0cC3sTnEjUNn8lrz2F+7j40AACAG5wtHswIAABwMQQiAABgewQiAABgewQiAABgewQiXFOrVq1S37591aBBAzkcDuXl5Xm7JdjcK6+8oubNm6t27dqKiYnRp59+6u2WYGOffPKJBg4cqMjISDkcDr3zzjvebsk2CES4pk6dOqXbb79dzz//vLdbAfT2228rJSVFU6dO1RdffKE777xT/fv31/79+73dGmzq1KlT6tixo9LS0rzdiu1w2z284ttvv1Xz5s31xRdfqFOnTt5uBzYVGxurW2+9VfPnz7fG2rRpo3vvvVczZ870YmfAT3+QPCMjQ/fee6+3W7EFZogA2FJpaalyc3PVp08fj/E+ffpo48aNXuoKgLcQiADY0vfff6+ysjK5XC6PcZfLpfz8fC91BcBbCES4apYvX666detaCxeroiZyOBwe68aYCmMAbny2+ltmuLYSExMVGxtrrd90001e7Abw1KBBA/n4+FSYDSooKKgwawTgxkcgwlUTHBys4OBgb7cBVMrf318xMTHKysrSfffdZ41nZWXpnnvu8WJnALyBQIRr6ocfftD+/ft16NAhSdLu3bslSREREYqIiPBma7Ch8ePHKzk5WV26dFFcXJxee+017d+/X4899pi3W4NNnTx5Ul9//bW1vnfvXuXl5Sk0NFRNmjTxYmc3Pm67xzWVnp6u3/3udxXGp02bpunTp1/7hmB7r7zyimbPnq3Dhw+rXbt2mjt3rn796197uy3Y1Mcff6yePXtWGB86dKjS09OvfUM2QiACAAC2x11mAADA9ghEAADA9ghEAADA9ghEAADA9ghEAADA9ghEAADA9ghEAADA9ghEAK4Ih8Ohd955x9ttXLd69OihlJSUX7SP9PR01atX74r0A9gNgQjAReXn5+uJJ55Qy5YtVbt2bblcLt1xxx169dVXdfr0aW+3V2MQCIHrG3/LDECVvvnmG91+++2qV6+eUlNT1b59e509e1b/+te/9MYbbygyMlKJiYnebhMAfjFmiABUafTo0fL19dXWrVs1ePBgtWnTRu3bt9f999+v1atXa+DAgVW+dvLkyWrVqpXq1KmjFi1a6M9//rPOnDljbX/00Ud17733erwmJSVFPXr0sNbPnTunWbNmqWXLlgoICFCTJk30//7f/7O2b9++XXfddZcCAwMVFhamkSNH6uTJkxWOkZqaKpfLpXr16mnGjBk6e/asnnrqKYWGhqpx48Z64403Lqn3S3Xs2DE9/PDDaty4serUqaP27dvrrbfeqlB39uxZjRkzRvXq1VNYWJj+9Kc/6fy/rlRaWqpJkybppptuUlBQkGJjY/Xxxx9Xedx//vOf6tmzp4KDgxUSEqKYmBht3br1ss8DuJExQwSgUseOHdOaNWuUmpqqoKCgSmscDkeVrw8ODlZ6eroiIyO1fft2jRgxQsHBwZo0aVK1e5gyZYoWLlyouXPn6o477tDhw4f11VdfSZJOnz6tfv36qWvXrsrJyVFBQYGGDx+uMWPGePwRzHXr1qlx48b65JNP9Nlnn2nYsGHatGmTfv3rX2vz5s16++239dhjj6l3796Kioq6Yr2f78cff1RMTIwmT56skJAQrV69WsnJyWrRooViY2OtuiVLlmjYsGHavHmztm7dqpEjR6pp06YaMWKEJOl3v/udvv32W61cuVKRkZHKyMhQv379tH37dkVHR1c47pAhQ9S5c2fNnz9fPj4+ysvLk5+f32WdA3DDMwBQiezsbCPJrFq1ymM8LCzMBAUFmaCgIDNp0iRrXJLJyMiocn+zZ882MTEx1vrQoUPNPffc41HzxBNPmO7duxtjjCkqKjIBAQFm4cKFle7vtddeM/Xr1zcnT560xlavXm1q1apl8vPzrWM0bdrUlJWVWTWtW7c2d955p7V+9uxZExQUZN56661q916Znzv/C919991mwoQJ1nr37t1NmzZtzLlz56yxyZMnmzZt2hhjjPn666+Nw+Ew3333ncd+4uPjzZQpU4wxxixevNg4nU5rW3BwsElPT692T4CdMUME4KIunAXasmWLzp07pyFDhqikpKTK1/3tb3/TvHnz9PXXX+vkyZM6e/asQkJCqn3cXbt2qaSkRPHx8VVu79ixo8fs1e23365z585p9+7dcrlckqRbbrlFtWr939UBLpdL7dq1s9Z9fHwUFhamgoKCK9b7hcrKyvT888/r7bff1nfffaeSkhKVlJRUmHnr2rWrx/sdFxenF198UWVlZfr8889ljFGrVq08XlNSUqKwsLBKjzt+/HgNHz5cb775pnr16qXf/OY3uvnmmy/7PIAbGdcQAahUy5Yt5XA4rK+oyrVo0UItW7ZUYGBgla/Nzs7WQw89pP79++v999/XF198oalTp6q0tNSqqVWrlsf1MZI8rtO52P4lyRhT5Vd2549f+BWRw+GodOzcuXPV7v1Svfjii5o7d64mTZqkdevWKS8vT3379r2kfZ47d04+Pj7Kzc1VXl6etezatUsvvfRSpa+ZPn26du7cqQEDBmjdunVq27atMjIyLvs8gBsZgQhApcLCwtS7d2+lpaXp1KlTl/Tazz77TE2bNtXUqVPVpUsXRUdHa9++fR41DRs21OHDhz3G8vLyrP+Ojo5WYGCgPvzww0qP0bZtW+Xl5Xn09tlnn6lWrVoVZlGudO+X6tNPP9U999yj3/72t+rYsaNatGihPXv2VKjLzs6usB4dHS0fHx917txZZWVlKigoUMuWLT2WiIiIKo/dqlUrPfnkk1qzZo0GDRqkxYsX/6JzAW5UBCIAVXrllVd09uxZdenSRW+//bZ27dql3bt3a9myZfrqq6/k4+NT6etatmyp/fv3a+XKlfrf//1f/eUvf6kwM3HXXXdp69atWrp0qfbs2aNp06Zpx44d1vbatWtr8uTJmjRpkpYuXar//d//VXZ2thYtWiTppwuGa9euraFDh2rHjh366KOPNHbsWCUnJ1tfl12O6vRelb1793rM3uTl5enkyZNq2bKlsrKytHHjRu3atUujRo1Sfn5+hdcfOHBA48eP1+7du/XWW2/p5Zdf1hNPPCHpp2AzZMgQPfLII1q1apX27t2rnJwczZo1Sx988EGFfRUXF2vMmDH6+OOPtW/fPn322WfKyclRmzZtLvu9AW5oXr6GCUANd+jQITNmzBjTvHlz4+fnZ+rWrWt+9atfmRdeeMGcOnXKqtMFFxU/9dRTJiwszNStW9c8+OCDZu7cuR4X/BpjzDPPPGNcLpdxOp3mySefNGPGjLEuqjbGmLKyMvPcc8+Zpk2bGj8/P9OkSROTmppqbd+2bZvp2bOnqV27tgkNDTUjRowwJ06csLZXduF29+7dzRNPPOEx1rRpUzN37txL6v1CkipdPvroI3Ps2DFzzz33mLp165rw8HDzpz/9yTzyyCMevXXv3t2MHj3aPPbYYyYkJMTUr1/f/PGPf/S4yLq0tNQ888wzplmzZsbPz89ERESY++67z2zbts0Y43lRdUlJiXnooYdMVFSU8ff3N5GRkWbMmDGmuLj4oucB2JXDmAu+xAcAALAZvjIDAAC2RyACAAC2RyACAAC2RyACAAC2RyACAAC2RyACAAC2RyACAAC2RyACAAC2RyACAAC2RyACAAC2RyACAAC2RyACAAC29/8BOqS/No/u3PYAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1 -1]\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "# plot the 'types' column (glaucoma labels)\n",
    "sns.countplot(x='types', data=df)\n",
    "plt.title('Distribution of Glaucoma Labels')\n",
    "plt.xlabel('Glaucoma Labels')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "# print the unique values in the 'types' column\n",
    "print(df['types'].unique())\n",
    "# drop all the -1 types values from the df\n",
    "df = df[df['types'] != -1]\n",
    "# print the unique values in the 'types' column\n",
    "print(df['types'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Modified U-Net for multi-task segmentation\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class MultiTaskUNet(nn.Module):\n",
    "    def __init__(self, n_channels=3, n_classes=2):  # n_classes=2 for OC and OD\n",
    "        super().__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        # Encoder (shared)\n",
    "        self.inc = DoubleConv(n_channels, 64)\n",
    "        self.down1 = nn.Sequential(nn.MaxPool2d(2), DoubleConv(64, 128))\n",
    "        self.down2 = nn.Sequential(nn.MaxPool2d(2), DoubleConv(128, 256))\n",
    "        self.down3 = nn.Sequential(nn.MaxPool2d(2), DoubleConv(256, 512))\n",
    "        self.down4 = nn.Sequential(nn.MaxPool2d(2), DoubleConv(512, 1024))\n",
    "\n",
    "        # Decoder for Optic Cup\n",
    "        self.up1_oc = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        self.up_conv1_oc = DoubleConv(1024, 512)\n",
    "        self.up2_oc = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.up_conv2_oc = DoubleConv(512, 256)\n",
    "        self.up3_oc = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.up_conv3_oc = DoubleConv(256, 128)\n",
    "        self.up4_oc = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.up_conv4_oc = DoubleConv(128, 64)\n",
    "        self.outc_oc = nn.Conv2d(64, 1, kernel_size=1)\n",
    "        \n",
    "        # Decoder for Optic Disc\n",
    "        self.up1_od = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        self.up_conv1_od = DoubleConv(1024, 512)\n",
    "        self.up2_od = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.up_conv2_od = DoubleConv(512, 256)\n",
    "        self.up3_od = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.up_conv3_od = DoubleConv(256, 128)\n",
    "        self.up4_od = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.up_conv4_od = DoubleConv(128, 64)\n",
    "        self.outc_od = nn.Conv2d(64, 1, kernel_size=1)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shared encoder\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "\n",
    "        # Optic Cup decoder\n",
    "        x_oc = self.up1_oc(x5)\n",
    "        x_oc = self.up_conv1_oc(torch.cat([x4, x_oc], dim=1))\n",
    "        x_oc = self.up2_oc(x_oc)\n",
    "        x_oc = self.up_conv2_oc(torch.cat([x3, x_oc], dim=1))\n",
    "        x_oc = self.up3_oc(x_oc)\n",
    "        x_oc = self.up_conv3_oc(torch.cat([x2, x_oc], dim=1))\n",
    "        x_oc = self.up4_oc(x_oc)\n",
    "        x_oc = self.up_conv4_oc(torch.cat([x1, x_oc], dim=1))\n",
    "        x_oc = self.outc_oc(x_oc)\n",
    "        oc_mask = self.sigmoid(x_oc)\n",
    "        \n",
    "        # Optic Disc decoder\n",
    "        x_od = self.up1_od(x5)\n",
    "        x_od = self.up_conv1_od(torch.cat([x4, x_od], dim=1))\n",
    "        x_od = self.up2_od(x_od)\n",
    "        x_od = self.up_conv2_od(torch.cat([x3, x_od], dim=1))\n",
    "        x_od = self.up3_od(x_od)\n",
    "        x_od = self.up_conv3_od(torch.cat([x2, x_od], dim=1))\n",
    "        x_od = self.up4_od(x_od)\n",
    "        x_od = self.up_conv4_od(torch.cat([x1, x_od], dim=1))\n",
    "        x_od = self.outc_od(x_od)\n",
    "        od_mask = self.sigmoid(x_od)\n",
    "        \n",
    "        return oc_mask, od_mask\n",
    "\n",
    "# Enhanced dataset class for both OC and OD masks\n",
    "class FundusMultiTaskDataset(Dataset):\n",
    "    def __init__(self, df, base_path, transform=None):\n",
    "        self.df = df\n",
    "        self.base_path = base_path\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get image name from the path\n",
    "        fundus_name = self.df.iloc[idx]['fundus'].split('/')[-1]\n",
    "        oc_mask_name = self.df.iloc[idx]['fundus_oc_seg'].split('/')[-1]\n",
    "        od_mask_name = self.df.iloc[idx]['fundus_od_seg'].split('/')[-1]\n",
    "        \n",
    "        # Construct full paths using the correct directory structure\n",
    "        fundus_path = os.path.join(self.base_path, 'full-fundus', 'full-fundus', fundus_name)\n",
    "        oc_mask_path = os.path.join(self.base_path, 'optic-cup', 'optic-cup', oc_mask_name)\n",
    "        od_mask_path = os.path.join(self.base_path, 'optic-disc', 'optic-disc', od_mask_name)\n",
    "\n",
    "        # Load images\n",
    "        image = Image.open(fundus_path).convert('RGB')\n",
    "        oc_mask = Image.open(oc_mask_path).convert('L')\n",
    "        od_mask = Image.open(od_mask_path).convert('L')\n",
    "\n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            oc_mask = self.transform(oc_mask)\n",
    "            od_mask = self.transform(od_mask)\n",
    "\n",
    "        return image, oc_mask, od_mask\n",
    "\n",
    "# Combined loss function\n",
    "def multi_task_dice_loss(pred_oc, target_oc, pred_od, target_od, alpha=0.5):\n",
    "    smooth = 1e-5\n",
    "    \n",
    "    # Dice loss for optic cup\n",
    "    intersection_oc = (pred_oc * target_oc).sum()\n",
    "    dice_oc = 1 - ((2. * intersection_oc + smooth) / (pred_oc.sum() + target_oc.sum() + smooth))\n",
    "    \n",
    "    # Dice loss for optic disc\n",
    "    intersection_od = (pred_od * target_od).sum()\n",
    "    dice_od = 1 - ((2. * intersection_od + smooth) / (pred_od.sum() + target_od.sum() + smooth))\n",
    "    \n",
    "    # Weighted sum of both losses\n",
    "    return alpha * dice_oc + (1 - alpha) * dice_od, dice_oc.item(), dice_od.item()\n",
    "\n",
    "# Enhanced training function for multi-task learning\n",
    "def train_multi_task_model(model, train_loader, val_loader, device, alpha=0.5, num_epochs=100, \n",
    "                           continue_from=None, log_dir='training_logs'):\n",
    "    # Create log directory if it doesn't exist\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    \n",
    "    # Initialize or load model state\n",
    "    start_epoch = 0\n",
    "    best_val_loss = float('inf')\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_oc_losses = []\n",
    "    train_od_losses = []\n",
    "    val_oc_losses = []\n",
    "    val_od_losses = []\n",
    "    \n",
    "    if continue_from and os.path.exists(continue_from):\n",
    "        print(f\"Loading model state from {continue_from}\")\n",
    "        checkpoint = torch.load(continue_from)\n",
    "        \n",
    "        # Handle checkpoint format\n",
    "        if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            start_epoch = checkpoint.get('epoch', 0)\n",
    "            best_val_loss = checkpoint.get('best_val_loss', float('inf'))\n",
    "            # Load history if available\n",
    "            train_losses = checkpoint.get('train_losses', [])\n",
    "            val_losses = checkpoint.get('val_losses', [])\n",
    "            train_oc_losses = checkpoint.get('train_oc_losses', [])\n",
    "            train_od_losses = checkpoint.get('train_od_losses', [])\n",
    "            val_oc_losses = checkpoint.get('val_oc_losses', [])\n",
    "            val_od_losses = checkpoint.get('val_od_losses', [])\n",
    "        else:\n",
    "            # Legacy format (just model state)\n",
    "            model.load_state_dict(checkpoint)\n",
    "            \n",
    "        print(f\"Continuing training from epoch {start_epoch}\")\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "    \n",
    "    # Create log file\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    log_file = os.path.join(log_dir, f'training_log_{timestamp}.txt')\n",
    "    \n",
    "    # Log initial info\n",
    "    with open(log_file, 'w') as f:\n",
    "        f.write(\"Multi-Task Training Started\\n\")\n",
    "        f.write(\"=\" * 100 + \"\\n\")\n",
    "        f.write(f\"{'Epoch':^6} | {'Batch':^12} | {'Train Loss':^12} | {'OC Loss':^12} | {'OD Loss':^12} | {'Val Loss':^12} | {'Val OC':^12} | {'Val OD':^12} | {'Best':^6}\\n\")\n",
    "        f.write(\"-\" * 100 + \"\\n\")\n",
    "    \n",
    "    print(\"\\nMulti-Task Training Started\")\n",
    "    print(\"=\" * 100)\n",
    "    print(f\"{'Epoch':^6} | {'Batch':^12} | {'Train Loss':^12} | {'OC Loss':^12} | {'OD Loss':^12} | {'Val Loss':^12} | {'Val OC':^12} | {'Val OD':^12} | {'Best':^6}\")\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "    for epoch in range(start_epoch, start_epoch + num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_oc_loss = 0\n",
    "        train_od_loss = 0\n",
    "        \n",
    "        for batch_idx, (images, oc_masks, od_masks) in enumerate(train_loader):\n",
    "            images = images.to(device)\n",
    "            oc_masks = oc_masks.to(device)\n",
    "            od_masks = od_masks.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            oc_outputs, od_outputs = model(images)\n",
    "            \n",
    "            loss, oc_loss, od_loss = multi_task_dice_loss(\n",
    "                oc_outputs, oc_masks, od_outputs, od_masks, alpha=alpha\n",
    "            )\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_oc_loss += oc_loss\n",
    "            train_od_loss += od_loss\n",
    "            \n",
    "            # Print batch progress\n",
    "            if (batch_idx + 1) % 10 == 0:  # Print every 10 batches\n",
    "                current_train_loss = train_loss / (batch_idx + 1)\n",
    "                current_oc_loss = train_oc_loss / (batch_idx + 1)\n",
    "                current_od_loss = train_od_loss / (batch_idx + 1)\n",
    "                \n",
    "                batch_log = f\"{epoch+1:^6d} | {f'{batch_idx+1}/{len(train_loader)}':^12} | {current_train_loss:^12.4f} | {current_oc_loss:^12.4f} | {current_od_loss:^12.4f} | {'---':^12} | {'---':^12} | {'---':^12} | {'':^6}\"\n",
    "                print(batch_log, end='\\r')\n",
    "                with open(log_file, 'a') as f:\n",
    "                    f.write(batch_log + '\\n')\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_oc_loss = 0\n",
    "        val_od_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, oc_masks, od_masks in val_loader:\n",
    "                images = images.to(device)\n",
    "                oc_masks = oc_masks.to(device)\n",
    "                od_masks = od_masks.to(device)\n",
    "                \n",
    "                oc_outputs, od_outputs = model(images)\n",
    "                \n",
    "                batch_loss, oc_loss, od_loss = multi_task_dice_loss(\n",
    "                    oc_outputs, oc_masks, od_outputs, od_masks, alpha=alpha\n",
    "                )\n",
    "                \n",
    "                val_loss += batch_loss.item()\n",
    "                val_oc_loss += oc_loss\n",
    "                val_od_loss += od_loss\n",
    "\n",
    "        # Average losses\n",
    "        train_loss /= len(train_loader)\n",
    "        train_oc_loss /= len(train_loader)\n",
    "        train_od_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        val_oc_loss /= len(val_loader)\n",
    "        val_od_loss /= len(val_loader)\n",
    "        \n",
    "        # Append to history\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_oc_losses.append(train_oc_loss)\n",
    "        train_od_losses.append(train_od_loss)\n",
    "        val_oc_losses.append(val_oc_loss)\n",
    "        val_od_losses.append(val_od_loss)\n",
    "\n",
    "        # Check if this is the best model\n",
    "        is_best = val_loss < best_val_loss\n",
    "        if is_best:\n",
    "            best_val_loss = val_loss\n",
    "            # Save checkpoint\n",
    "            save_path = os.path.join(log_dir, 'best_multitask_model.pth')\n",
    "            checkpoint = {\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_val_loss': best_val_loss,\n",
    "                'train_losses': train_losses,\n",
    "                'val_losses': val_losses,\n",
    "                'train_oc_losses': train_oc_losses,\n",
    "                'train_od_losses': train_od_losses,\n",
    "                'val_oc_losses': val_oc_losses,\n",
    "                'val_od_losses': val_od_losses,\n",
    "                'timestamp': datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            }\n",
    "            torch.save(checkpoint, save_path)\n",
    "\n",
    "        # Log progress with epoch summary\n",
    "        log_line = f\"{epoch+1:^6d} | {'Complete':^12} | {train_loss:^12.4f} | {train_oc_loss:^12.4f} | {train_od_loss:^12.4f} | {val_loss:^12.4f} | {val_oc_loss:^12.4f} | {val_od_loss:^12.4f} | {'*' if is_best else ' ':^6}\"\n",
    "        print(log_line)\n",
    "        with open(log_file, 'a') as f:\n",
    "            f.write(log_line + '\\n')\n",
    "\n",
    "    print(\"=\" * 100)\n",
    "    final_msg = f\"Training completed. Best validation loss: {best_val_loss:.4f}\"\n",
    "    print(final_msg)\n",
    "    with open(log_file, 'a') as f:\n",
    "        f.write(\"=\" * 100 + '\\n')\n",
    "        f.write(final_msg + '\\n')\n",
    "    \n",
    "    # Return all histories for plotting\n",
    "    return {\n",
    "        'train_loss': train_losses,\n",
    "        'val_loss': val_losses,\n",
    "        'train_oc_loss': train_oc_losses,\n",
    "        'train_od_loss': train_od_losses,\n",
    "        'val_oc_loss': val_oc_losses,\n",
    "        'val_od_loss': val_od_losses\n",
    "    }\n",
    "\n",
    "# Enhanced visualization function\n",
    "def plot_training_history(history):\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Plot combined losses\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(history['train_loss'], label='Combined Train Loss')\n",
    "    plt.plot(history['val_loss'], label='Combined Val Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Combined Training and Validation Loss')\n",
    "    \n",
    "    # Plot task-specific losses\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(history['train_oc_loss'], label='OC Train Loss')\n",
    "    plt.plot(history['val_oc_loss'], label='OC Val Loss')\n",
    "    plt.plot(history['train_od_loss'], label='OD Train Loss', linestyle='--')\n",
    "    plt.plot(history['val_od_loss'], label='OD Val Loss', linestyle='--')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Task-Specific Training and Validation Loss')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_history.png')\n",
    "    plt.show()\n",
    "\n",
    "# Function to visualize predictions\n",
    "def visualize_predictions(model, val_loader, device, num_samples=3):\n",
    "    model.eval()\n",
    "    plt.figure(figsize=(15, 5 * num_samples))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (images, oc_masks, od_masks) in enumerate(val_loader):\n",
    "            if i >= num_samples:\n",
    "                break\n",
    "                \n",
    "            images = images.to(device)\n",
    "            oc_preds, od_preds = model(images)\n",
    "            \n",
    "            # Move tensors to CPU for visualization\n",
    "            images = images.cpu()\n",
    "            oc_masks = oc_masks.cpu()\n",
    "            od_masks = od_masks.cpu()\n",
    "            oc_preds = oc_preds.cpu()\n",
    "            od_preds = od_preds.cpu()\n",
    "            \n",
    "            # Plot for each image in the batch\n",
    "            for j in range(min(1, images.size(0))):  # Plot only first image in each batch\n",
    "                plt.subplot(num_samples, 5, i*5 + 1)\n",
    "                plt.imshow(images[j].permute(1, 2, 0))\n",
    "                plt.title('Original Image')\n",
    "                plt.axis('off')\n",
    "                \n",
    "                plt.subplot(num_samples, 5, i*5 + 2)\n",
    "                plt.imshow(oc_masks[j].squeeze(), cmap='gray')\n",
    "                plt.title('GT Optic Cup')\n",
    "                plt.axis('off')\n",
    "                \n",
    "                plt.subplot(num_samples, 5, i*5 + 3)\n",
    "                plt.imshow(oc_preds[j].squeeze(), cmap='gray')\n",
    "                plt.title('Pred Optic Cup')\n",
    "                plt.axis('off')\n",
    "                \n",
    "                plt.subplot(num_samples, 5, i*5 + 4)\n",
    "                plt.imshow(od_masks[j].squeeze(), cmap='gray')\n",
    "                plt.title('GT Optic Disc')\n",
    "                plt.axis('off')\n",
    "                \n",
    "                plt.subplot(num_samples, 5, i*5 + 5)\n",
    "                plt.imshow(od_preds[j].squeeze(), cmap='gray')\n",
    "                plt.title('Pred Optic Disc')\n",
    "                plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('prediction_samples.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 2244\n",
      "Validation samples: 561\n",
      "\n",
      "Multi-Task Training Started\n",
      "====================================================================================================\n",
      "Epoch  |    Batch     |  Train Loss  |   OC Loss    |   OD Loss    |   Val Loss   |    Val OC    |    Val OD    |  Best \n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:\\\\glaucoma\\\\data\\\\raw\\\\SMDG-19\\\\metadata - standardized.csv\\\\full-fundus\\\\full-fundus\\\\G1020-904.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 45\u001b[0m\n\u001b[0;32m     42\u001b[0m alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_multi_task_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontinue_from\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Set to path if continuing from saved model\u001b[39;49;00m\n\u001b[0;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_dir\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Plot detailed training history\u001b[39;00m\n\u001b[0;32m     54\u001b[0m plot_training_history(history)\n",
      "Cell \u001b[1;32mIn[13], line 214\u001b[0m, in \u001b[0;36mtrain_multi_task_model\u001b[1;34m(model, train_loader, val_loader, device, alpha, num_epochs, continue_from, log_dir)\u001b[0m\n\u001b[0;32m    211\u001b[0m train_oc_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    212\u001b[0m train_od_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 214\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moc_masks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mod_masks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m    \u001b[49m\u001b[43moc_masks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moc_masks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Michi\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Michi\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Michi\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[13], line 124\u001b[0m, in \u001b[0;36mFundusMultiTaskDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    121\u001b[0m od_mask_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptic-disc\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptic-disc\u001b[39m\u001b[38;5;124m'\u001b[39m, od_mask_name)\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# Load images\u001b[39;00m\n\u001b[1;32m--> 124\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfundus_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    125\u001b[0m oc_mask \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(oc_mask_path)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    126\u001b[0m od_mask \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(od_mask_path)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Michi\\anaconda3\\Lib\\site-packages\\PIL\\Image.py:3277\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3274\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mrealpath(os\u001b[38;5;241m.\u001b[39mfspath(fp))\n\u001b[0;32m   3276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[1;32m-> 3277\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3278\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   3280\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:\\\\glaucoma\\\\data\\\\raw\\\\SMDG-19\\\\metadata - standardized.csv\\\\full-fundus\\\\full-fundus\\\\G1020-904.png'"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Load and prepare your DataFrame\n",
    "base_path = r'D:\\glaucoma\\data\\raw\\SMDG-19\\metadata - standardized.csv' \n",
    "\n",
    "# Remove rows with NaN in required columns and \"Not Visible\" values\n",
    "df = df.dropna(subset=['fundus', 'fundus_oc_seg', 'fundus_od_seg'])\n",
    "df = df[df['fundus_oc_seg'] != 'Not Visible']\n",
    "df = df[df['fundus_od_seg'] != 'Not Visible']\n",
    "\n",
    "# Split data\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print dataset sizes\n",
    "print(f'Train samples: {len(train_df)}')\n",
    "print(f'Validation samples: {len(val_df)}')\n",
    "\n",
    "# Define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Create datasets with both OC and OD masks\n",
    "train_dataset = FundusMultiTaskDataset(train_df, base_path, transform=transform)\n",
    "val_dataset = FundusMultiTaskDataset(val_df, base_path, transform=transform)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8)\n",
    "\n",
    "# Initialize multi-task model and move to device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = MultiTaskUNet().to(device)\n",
    "\n",
    "# Create log directory\n",
    "log_dir = 'multitask_training_logs'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Set loss weighting parameter (alpha=0.5 gives equal weight to both tasks)\n",
    "alpha = 0.5\n",
    "\n",
    "# Train the model\n",
    "history = train_multi_task_model(\n",
    "    model, train_loader, val_loader, device, \n",
    "    alpha=alpha,\n",
    "    num_epochs=100, \n",
    "    continue_from=None,  # Set to path if continuing from saved model\n",
    "    log_dir=log_dir\n",
    ")\n",
    "\n",
    "# Plot detailed training history\n",
    "plot_training_history(history)\n",
    "\n",
    "# Visualize some predictions\n",
    "visualize_predictions(model, val_loader, device, num_samples=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results on validation set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Get a batch of validation images\n",
    "    images, masks = next(iter(val_loader))\n",
    "    images, masks = images.to(device), masks.to(device)\n",
    "    \n",
    "    # Get predictions\n",
    "    preds = model(images)\n",
    "    \n",
    "    # Plot results for nine images\n",
    "    fig, axes = plt.subplots(9, 3, figsize=(15, 45))  # Adjusted figure size for 9 rows\n",
    "    \n",
    "    for idx in range(min(9, len(images))):\n",
    "        # Original image\n",
    "        axes[idx, 0].imshow(images[idx].cpu().permute(1, 2, 0))\n",
    "        axes[idx, 0].set_title('Original Image')\n",
    "        axes[idx, 0].axis('off')\n",
    "        \n",
    "        # Ground truth mask\n",
    "        axes[idx, 1].imshow(masks[idx].cpu().squeeze(), cmap='gray')\n",
    "        axes[idx, 1].set_title('Ground Truth')\n",
    "        axes[idx, 1].axis('off')\n",
    "        \n",
    "        # Predicted mask\n",
    "        axes[idx, 2].imshow(preds[idx].cpu().squeeze(), cmap='gray')\n",
    "        axes[idx, 2].set_title('Prediction')\n",
    "        axes[idx, 2].axis('off')\n",
    "    \n",
    "    # Add column labels\n",
    "    fig.suptitle('Validation Results: Original vs Ground Truth vs Prediction', y=0.92)\n",
    "    axes[0, 0].set_title('Original Image', pad=20, fontsize=12)\n",
    "    axes[0, 1].set_title('Ground Truth', pad=20, fontsize=12)\n",
    "    axes[0, 2].set_title('Prediction', pad=20, fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot summury statistics of following columns: gender, age, eye, iop, vcdr,  'expert1_grade', 'expert2_grade', 'expert3_grade', 'expert4_grade', 'expert5_grade''cdr_avg', 'cdr_expert1','cdr_expert2', 'cdr_expert3', 'cdr_expert4\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set of relevant columns\n",
    "cols = ['gender', 'age', 'eye', 'iop', 'vcdr',\n",
    "        'expert1_grade', 'expert2_grade', 'expert3_grade', 'expert4_grade', 'expert5_grade',\n",
    "        'cdr_avg', 'cdr_expert1', 'cdr_expert2', 'cdr_expert3', 'cdr_expert4']\n",
    "\n",
    "# Total number of rows in the dataset\n",
    "total_rows = len(df)\n",
    "\n",
    "# Set plot style\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(16, 20))\n",
    "\n",
    "# Loop through columns and plot each one\n",
    "for i, col in enumerate(cols, 1):\n",
    "    plt.subplot(5, 3, i)\n",
    "    non_null_count = df[col].notnull().sum()\n",
    "    title = f\"{col} ({non_null_count}/{total_rows} filled)\"\n",
    "    \n",
    "    if df[col].dtype == 'object' or df[col].nunique() < 10:\n",
    "        # Categorical: countplot\n",
    "        sns.countplot(data=df, x=col, order=df[col].dropna().value_counts().index)\n",
    "        plt.xticks(rotation=45)\n",
    "    else:\n",
    "        # Numerical: histogram\n",
    "        sns.histplot(df[col].dropna(), kde=True, bins=20)\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.suptitle(\"Summary Statistics of Glaucoma Dataset (Filled Counts Included)\", fontsize=16, y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_airogs = pd.read_csv(r'D:\\glaucoma_datasets\\AIROGS\\train_labels.csv')\n",
    "df_airogs.head()\n",
    "\n",
    "# path to the images is D:\\glaucoma_datasets\\AIROGS\\img + challenge_id + .jpg\n",
    "df_airogs['path'] = df_airogs['challenge_id'].apply(lambda x: os.path.join(r'D:\\glaucoma_datasets\\AIROGS\\img', str(x) + '.jpg'))\n",
    "# check the class distribution\n",
    "df_airogs['class'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PAPILLA dataset: already included in SMDG-19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlaucomaDataset(Dataset):\n",
    "    def __init__(self, root_dir, csv_file, image_dir='Images', transform=None, max_images=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.csv_file = csv_file\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.max_images = max_images\n",
    "        \n",
    "        if csv_file == \"index.json\":\n",
    "            with open(os.path.join(root_dir, csv_file), 'r') as f:\n",
    "                self.labels_dict = json.load(f)\n",
    "            self.image_filenames = list(self.labels_dict.keys())\n",
    "        elif csv_file == \"metadata - standardized.csv\":\n",
    "            self.labels_df = pd.read_csv(os.path.join(root_dir, csv_file)) \n",
    "            # Filter DataFrame based on labels\n",
    "            self.labels_df = self.labels_df[self.labels_df['types'].isin([0, 1])]\n",
    "            \n",
    "            \n",
    "            # Filter image_filenames based on labels\n",
    "            self.image_filenames = [f for f in os.listdir(os.path.join(root_dir, image_dir)) if f.endswith('.png')]\n",
    "            self.image_filenames = [f for f in self.image_filenames if f[:-4] in self.labels_df['names'].values]\n",
    "\n",
    " \n",
    "        else:\n",
    "            self.labels_df = pd.read_csv(os.path.join(root_dir, csv_file))\n",
    "            self.image_filenames = [f for f in os.listdir(os.path.join(root_dir, image_dir)) if f.endswith('.jpg')]\n",
    "            \n",
    "        \n",
    "        self.image_filenames = self.image_filenames[:max_images] if max_images is not None else self.image_filenames\n",
    "\n",
    "        print(f'Successfully loaded dataset with {len(self.image_filenames)} images.')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, self.image_dir, self.image_filenames[idx])\n",
    "        img = cv2.imread(img_name)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "        img = Image.fromarray(img)  # Convert numpy array to PIL Image\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        if self.csv_file == 'metadata - standardized.csv':\n",
    "            label = self.labels_df.loc[self.labels_df['names']+\".png\" == self.image_filenames[idx], 'types'].values[0]        \n",
    "               \n",
    "            \n",
    "        label_tensor = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        \n",
    "        return img, label_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = Compose([\n",
    "    Resize((512,512)),  # Slightly larger to allow for random crops\n",
    "    RandomResizedCrop((312,312), scale=(0.8, 1.0)),  # Random scaling\n",
    "    RandomHorizontalFlip(),\n",
    "    RandomApply([RandomRotation(10)], p=0.5),\n",
    "    RandomApply([RandomAffine(degrees=0, scale=(0.9, 1.1))], p=0.5),  # Random scaling\n",
    "    Resize((312,312)),\n",
    "    \n",
    "    ToTensor(),\n",
    "   Normalize(mean=[0.7538, 0.4848, 0.3553], std=[0.2417, 0.1874, 0.2076]),\n",
    "])\n",
    "\n",
    "\n",
    "# Define validation transformations without augmentation\n",
    "val_transforms = Compose([\n",
    "    Resize((312,312)),\n",
    "    ToTensor(),\n",
    "   Normalize(mean=[0.7538, 0.4848, 0.3553], std=[0.2417, 0.1874, 0.2076]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "num_classes = 2\n",
    "\n",
    "smdg_csv_file='metadata - standardized.csv'\n",
    "smdg_root_dir = r'D:\\glaucoma_datasets\\SMDG-19'\n",
    "smdg_image_dir = r'full-fundus\\full-fundus'\n",
    "\n",
    "# check the number of files in the smdg_image_dir\n",
    "smdg_image_files = os.listdir(os.path.join(smdg_root_dir, smdg_image_dir))\n",
    "print(f\"Number of files in {smdg_image_dir}: {len(smdg_image_files)}\")\n",
    "\n",
    "\n",
    "# Initialize the datasets with appropriate transformations\n",
    "smdg_train_dataset = GlaucomaDataset(root_dir=smdg_root_dir, image_dir=smdg_image_dir,\n",
    "                                csv_file=smdg_csv_file, transform=train_transforms)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "smdg_val_dataset = GlaucomaDataset(root_dir=smdg_root_dir, image_dir=smdg_image_dir, \n",
    "                                      csv_file=smdg_csv_file, transform=val_transforms)\n",
    "\n",
    "\n",
    "\n",
    "# Split dataset indices\n",
    "smdg_train_indices, smdg_val_indices = train_test_split(range(len(smdg_train_dataset)), test_size=0.3, random_state=42)\n",
    "\n",
    "# Create Subset for train and validation datasets\n",
    "smdg_train_dataset = Subset(smdg_train_dataset, smdg_train_indices)\n",
    "smdg_val_dataset = Subset(smdg_val_dataset, smdg_val_indices)\n",
    "\n",
    "\n",
    "\n",
    "smdg_train_loader = DataLoader(smdg_train_dataset, batch_size=batch_size,\n",
    "                              shuffle=False, num_workers=os.cpu_count(), pin_memory=True)\n",
    "smdg_val_loader = DataLoader(smdg_val_dataset, batch_size=batch_size,\n",
    "                              shuffle=False, num_workers=os.cpu_count(), pin_memory=True)\n",
    "\n",
    "print(\"loaded smdg dataaset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loaders=[smdg_train_loader]\n",
    "\n",
    "val_loaders=[smdg_val_loader]\n",
    "\n",
    "dataset_name=[\"SMDG\"]\n",
    "\n",
    "# print the number of images in the train and validation datasets\n",
    "print(f\"Number of images in the train dataset: {len(smdg_train_dataset)}\")\n",
    "print(f\"Number of images in the validation dataset: {len(smdg_val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def show_images(dataset, num_images=5, title=\"Dataset Samples\"):\n",
    "    # Create a figure with num_images rows and 1 column\n",
    "    fig, axes = plt.subplots(num_images, 1, figsize=(8, 4*num_images))\n",
    "    \n",
    "    # Randomly select indices\n",
    "    indices = np.random.choice(len(dataset), num_images, replace=False)\n",
    "    \n",
    "    for idx, ax in zip(indices, axes):\n",
    "        image, label = dataset[idx]\n",
    "        \n",
    "        # Convert the image tensor to numpy and transpose to (H,W,C)\n",
    "        img_np = image.permute(1, 2, 0).numpy()\n",
    "        \n",
    "        # Normalize image for display if needed\n",
    "        img_np = (img_np - img_np.min()) / (img_np.max() - img_np.min())\n",
    "        \n",
    "        ax.imshow(img_np)\n",
    "        ax.axis('off')\n",
    "        ax.set_title(f'Label: {\"Glaucoma\" if label == 1 else \"Normal\"}')\n",
    "    \n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Display training samples\n",
    "print(\"Training Dataset Samples:\")\n",
    "show_images(smdg_train_dataset, title=\"Training Dataset Samples\")\n",
    "\n",
    "# Display validation samples\n",
    "print(\"\\nValidation Dataset Samples:\")\n",
    "show_images(smdg_val_dataset, title=\"Validation Dataset Samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.models import resnet50\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_history(history):\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 10))\n",
    "    \n",
    "    # Plot losses\n",
    "    ax1.plot(history['train_loss'], label='Train Loss')\n",
    "    ax1.plot(history['val_loss'], label='Val Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.set_title('Training and Validation Loss')\n",
    "    \n",
    "    # Plot accuracies\n",
    "    ax2.plot(history['train_acc'], label='Train Acc')\n",
    "    ax2.plot(history['val_acc'], label='Val Acc')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy (%)')\n",
    "    ax2.legend()\n",
    "    ax2.set_title('Training and Validation Accuracy')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, \n",
    "                num_epochs=50, device='cuda'):\n",
    "    \n",
    "    # Create directory for checkpoints\n",
    "    os.makedirs('checkpoints', exist_ok=True)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    history = {\n",
    "        'train_loss': [], 'train_acc': [],\n",
    "        'val_loss': [], 'val_acc': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        train_loop = tqdm(train_loader, desc=f'Epoch [{epoch+1}/{num_epochs}]')\n",
    "        for inputs, labels in train_loop:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            train_loop.set_postfix({\n",
    "                'loss': loss.item(), \n",
    "                'acc': 100 * train_correct / train_total\n",
    "            })\n",
    "        \n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_acc = 100 * train_correct / train_total\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_acc = 100 * val_correct / val_total\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Update history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        # Save checkpoint if validation loss improved\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': val_loss,\n",
    "            }, f'checkpoints/best_model.pth')\n",
    "            print(f'Saved new best model with validation loss: {val_loss:.4f}')\n",
    "        \n",
    "        print(f'\\nEpoch {epoch+1}/{num_epochs}:')\n",
    "        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
    "        print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "        print(f'Learning Rate: {optimizer.param_groups[0][\"lr\"]}')\n",
    "        print('-' * 60)\n",
    "    \n",
    "    # Plot training history\n",
    "    plot_training_history(history)\n",
    "    return model, history\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize model\n",
    "model = resnet50(weights='ResNet50_Weights.DEFAULT')\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)  # Modify last layer\n",
    "model = model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "\n",
    "# Train the model\n",
    "model, history = train_model(model, smdg_train_loader, smdg_val_loader, criterion, \n",
    "                           optimizer, scheduler, num_epochs=10, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
