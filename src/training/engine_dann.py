#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
DANN-Adapted Training Engine for PyTorch Models.

This engine handles the training loop for models, including Domain Adversarial
Neural Network (DANN) specific logic if enabled.
"""

import torch
import torch.nn as nn
import os
import time
from datetime import datetime
import logging
from tqdm import tqdm
import numpy as np
from torch.optim.lr_scheduler import ReduceLROnPlateau
import sys
from typing import Optional, Dict, Any

# Configure logger for this module
logger = logging.getLogger(__name__)
if not logger.hasHandlers(): # Avoid adding multiple handlers if imported multiple times
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

try:
    # Assuming plot_training_history is in a utils.plotting module relative to this engine_dann.py
    # Adjust the import path based on your actual project structure
    # For example, if engine_dann.py is in src/training/ and plotting.py is in src/utils/
    # from ..utils.plotting import plot_training_history
    # If they are in the same directory (less common for structured projects):
    # from .plotting import plot_training_history # (If plotting.py is in the same dir)

    # Placeholder if the exact relative import is tricky without full project context
    # You'll need to make sure this import works in your setup.
    # If plot_training_history is in src.utils.plotting and this is src.training.engine_dann:
    from src.utils.plotting import plot_training_history
except ImportError:
    plot_training_history = None # Define as None if import fails
    logger.warning(
        "plot_training_history function not found or import failed. "
        "Training history plots will not be generated by the engine."
    )


def train_model_dann(model: nn.Module,
                     train_loader: torch.utils.data.DataLoader,
                     val_loader: Optional[torch.utils.data.DataLoader],
                     criterion_label: nn.Module,
                     optimizer: torch.optim.Optimizer,
                     scheduler: Optional[torch.optim.lr_scheduler._LRScheduler],
                     num_epochs: int,
                     device: torch.device,
                     checkpoint_dir: str,
                     experiment_name: str = "train_dann",
                     early_stopping_patience: Optional[int] = None,
                     use_amp: bool = False,
                     gradient_accumulation_steps: int = 1,
                     start_epoch: int = 0,
                     resume_history: Optional[Dict[str, Any]] = None,
                     # --- DANN Specific Arguments ---
                     use_dann: bool = False,
                     criterion_domain: Optional[nn.Module] = None,
                     dann_tradeoff_lambda: float = 1.0
                     ) -> tuple[nn.Module, Dict[str, Any]]:
    """
    Trains a PyTorch model, optionally with DANN capabilities.

    Args:
        model (nn.Module): PyTorch model to train. Should be DANNClassifierModel if use_dann=True.
        train_loader (DataLoader): DataLoader for training data.
                                   Expected to yield (images, labels, domain_ids, metadata) if DANN.
        val_loader (DataLoader, optional): DataLoader for validation data.
        criterion_label (nn.Module): Loss function for the main task (label prediction).
        optimizer (Optimizer): PyTorch optimizer.
        scheduler (LRScheduler, optional): Learning rate scheduler.
        num_epochs (int): TOTAL number of epochs the model should be trained for.
        device (torch.device): Device to train on (cpu or cuda).
        checkpoint_dir (str): Directory to save checkpoints.
        experiment_name (str): Name for this training run.
        early_stopping_patience (int, optional): Epochs to wait for early stopping. None to disable.
        use_amp (bool): Whether to use automatic mixed precision.
        gradient_accumulation_steps (int): Number of steps to accumulate gradients over.
        start_epoch (int): Epoch to start training from (0-indexed).
        resume_history (dict, optional): Existing history dict to append to if resuming.
        use_dann (bool): Whether to enable DANN training.
        criterion_domain (nn.Module, optional): Loss for domain classification. Required if use_dann.
        dann_tradeoff_lambda (float): Weight for the domain loss in the total loss.

    Returns:
        tuple: (trained_model, history_dict for the epochs trained IN THIS SESSION)
    """

    if train_loader is None:
        logger.error("Error: train_loader is None. Cannot start training.")
        return model, {}

    # --- Setup ---
    os.makedirs(checkpoint_dir, exist_ok=True)
    best_val_loss = float('inf')
    epochs_no_improve = 0
    
    history_this_session = {
        'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [], 'lr': [],
        'train_domain_loss': [], 'grl_lambda_values': [], 'dann_tradeoff_values': [] # DANN specific
    }
    
    best_epoch_this_session = -1

    # --- AMP Setup ---
    device_type = device.type
    amp_enabled = use_amp and (device_type == 'cuda')
    scaler = torch.cuda.amp.GradScaler(enabled=amp_enabled)
    amp_dtype = torch.float16 if device_type == 'cuda' and amp_enabled else None # Use float16 only if CUDA and AMP enabled
    logger.info(f"AMP enabled: {amp_enabled}" + (f" with dtype: {amp_dtype}" if amp_enabled else ""))


    # --- Checkpoint Paths ---
    best_model_path_template = os.path.join(checkpoint_dir, f'{experiment_name}_best_model_epoch{{}}.pth')
    last_checkpoint_path = os.path.join(checkpoint_dir, f'{experiment_name}_last_checkpoint.pth')

    # --- Gradient Accumulation Check ---
    if gradient_accumulation_steps < 1:
        logger.warning("gradient_accumulation_steps must be >= 1. Setting to 1.")
        gradient_accumulation_steps = 1
    if gradient_accumulation_steps > 1:
        logger.info(f"Using gradient accumulation with {gradient_accumulation_steps} steps.")

    # --- DANN Sanity Checks ---
    if use_dann:
        if criterion_domain is None:
            logger.error("DANN enabled but criterion_domain is None. Disabling DANN for this run.")
            use_dann = False
        # Check if the model has the necessary DANN attributes (like forward pass handling current_iter)
        # This check is heuristic; a more robust check might involve isinstance(model, DANNClassifierModel)
        if not (hasattr(model, 'grl') and hasattr(model, 'domain_classifier_head')):
             logger.warning("DANN enabled, but model may not be a DANNModel (missing grl or domain_classifier_head). "
                            "Ensure model's forward pass supports 'current_iter', 'total_iters', and 'mode' args.")
        else:
            logger.info(f"DANN training enabled. Domain loss tradeoff (alpha): {dann_tradeoff_lambda}")


    # --- Resume Logic ---
    if start_epoch > 0:
        logger.info(f"--- Resuming training from epoch {start_epoch + 1} (0-indexed start: {start_epoch}) ---")
        if resume_history and resume_history.get('val_loss'):
            valid_past_losses = [l for l in resume_history['val_loss'] if l is not None and np.isfinite(l)]
            if valid_past_losses:
                best_val_loss = min(valid_past_losses)
                logger.info(f"Initialized best_val_loss from resumed history: {best_val_loss:.4f}")
        epochs_no_improve = 0 # Reset for resumed phase, or load from checkpoint if saved

    def write_log(message, use_tqdm_write=False):
        if use_tqdm_write: tqdm.write(message, file=sys.stderr)
        else: logger.info(message)

    # --- Log Training Start Header ---
    if start_epoch == 0:
        patience_str = f"Patience: {early_stopping_patience}" if early_stopping_patience else "Patience: Disabled"
        amp_status_str = "AMP: Enabled" if amp_enabled else "AMP: Disabled"
        accum_str = f"Grad Accum: {gradient_accumulation_steps}" if gradient_accumulation_steps > 1 else "Grad Accum: Disabled"
        dann_str = f"DANN: {'Enabled, alpha=' + str(dann_tradeoff_lambda) if use_dann else 'Disabled'}"
        header_parts = [f"{'Epoch':^7}", f"{'Batch':^12}", f"{'LabelLoss':^11}", f"{'LabelAcc':^10}"]
        if use_dann: header_parts.extend([f"{'DomLoss':^9}", f"{'GRL_L':^7}"])
        header_parts.extend([f"{'ValLoss':^10}", f"{'ValAcc':^10}", f"{'LR':^8}", f"{'Best':^6}"])
        header = " | ".join(header_parts)
        separator = "-" * len(header)
        write_log("\n" + "=" * len(header))
        write_log(f"Training Started: {experiment_name} ({patience_str}, {amp_status_str}, {accum_str}, {dann_str})")
        write_log(separator); write_log(header); write_log(separator)

    overall_start_time = time.time()
    early_stop_triggered = False
    
    # Calculate total iterations for DANN's dynamic lambda scheduling
    loader_len = len(train_loader) if hasattr(train_loader, '__len__') and train_loader is not None else 1
    total_train_iters = num_epochs * loader_len
    if loader_len == 0 : # Handle empty train_loader case
        logger.error("train_loader has zero length. Cannot train.")
        return model, {}


    # --- Training Loop ---
    for epoch in range(start_epoch, num_epochs):
        current_epoch_display = epoch + 1
        epoch_start_time = time.time()
        write_log(f"--- Starting Epoch {current_epoch_display}/{num_epochs} ---")
        
        model.train()
        running_loss_label = 0.0
        running_loss_domain = 0.0 # DANN
        train_correct_label = 0
        train_total_samples = 0
        accumulated_grl_lambdas_epoch = []

        optimizer.zero_grad(set_to_none=True)

        batch_loop = tqdm(train_loader, desc=f"Epoch {current_epoch_display}/{num_epochs} Train", leave=False, unit="batch", disable=(loader_len == 0))
        for batch_idx, batch_data in enumerate(batch_loop):
            if batch_data is None:
                logger.warning(f"Skipping None batch_data at epoch {current_epoch_display}, batch {batch_idx+1}")
                continue

            current_iter_for_dann = epoch * loader_len + batch_idx

            try:
                if use_dann: # Expected: (images, labels, domain_ids, metadata)
                    if len(batch_data) != 4:
                        raise ValueError(f"DANN mode: Expected 4 items from train_loader, got {len(batch_data)}")
                    inputs, labels, domain_ids, _ = batch_data # Metadata ignored in engine
                    domain_ids = domain_ids.to(device, non_blocking=True)
                else: # Standard: (images, labels, metadata) or (images, labels)
                    if len(batch_data) == 3: inputs, labels, _ = batch_data
                    elif len(batch_data) == 2: inputs, labels = batch_data
                    else: raise ValueError(f"Standard mode: Expected 2 or 3 items, got {len(batch_data)}")
                
                inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)
            except (ValueError, TypeError) as e:
                 logger.error(f"Error unpacking train batch {batch_idx+1} in epoch {current_epoch_display}: {e}. Skipping.")
                 continue
            batch_size = inputs.size(0)
            if batch_size == 0: continue


            with torch.amp.autocast('cuda', enabled=amp_enabled, dtype=amp_dtype):
                total_loss_for_backward = torch.tensor(0.0, device=device)
                loss_l_val, loss_d_val, grl_lambda_this_batch = 0.0, 0.0, 0.0

                if use_dann:
                    # DANNModel forward: model(x, current_iter, total_iters, mode)
                    label_preds, domain_preds, grl_lambda_this_batch = model(
                        inputs, current_iter=current_iter_for_dann, total_iters=total_train_iters, mode='all'
                    )
                    accumulated_grl_lambdas_epoch.append(grl_lambda_this_batch)
                    
                    loss_l = criterion_label(label_preds, labels.long())
                    loss_d = criterion_domain(domain_preds, domain_ids.long())
                    total_loss_for_backward = loss_l + dann_tradeoff_lambda * loss_d
                    loss_l_val, loss_d_val = loss_l.item(), loss_d.item()
                else: # Standard training
                    # Standard model forward, or DANNModel in 'label_only' mode if it handles it
                    if hasattr(model, 'grl'): # Heuristic for DANNModel
                        label_preds = model(inputs, mode='label_only')
                    else:
                        label_preds = model(inputs)
                    
                    loss_l = criterion_label(label_preds, labels.long())
                    total_loss_for_backward = loss_l
                    loss_l_val = loss_l.item()

                if gradient_accumulation_steps > 1:
                    total_loss_for_backward = total_loss_for_backward / gradient_accumulation_steps
            
            if not torch.isfinite(total_loss_for_backward):
                 logger.warning(f"Non-finite total loss ({total_loss_for_backward.item()}) in E{current_epoch_display} B{batch_idx+1}. Skipping update.")
                 optimizer.zero_grad(set_to_none=True) 
                 continue 
            
            scaler.scale(total_loss_for_backward).backward()

            if (batch_idx + 1) % gradient_accumulation_steps == 0 or (batch_idx + 1) == loader_len:
                scaler.step(optimizer)
                scaler.update()
                optimizer.zero_grad(set_to_none=True)

            running_loss_label += loss_l_val * batch_size 
            if use_dann: running_loss_domain += loss_d_val * batch_size
            
            _, predicted_labels = torch.max(label_preds.data, 1)
            train_correct_label += (predicted_labels == labels).sum().item()
            train_total_samples += batch_size
            
            # TQDM postfix
            tqdm_postfix = {'L_loss': f"{loss_l_val:.3f}", 'L_acc': f"{100.*train_correct_label/train_total_samples if train_total_samples > 0 else 0:.1f}%"}
            if use_dann:
                tqdm_postfix['D_loss'] = f"{loss_d_val:.3f}"
                tqdm_postfix['GRL_L'] = f"{grl_lambda_this_batch:.2f}"
            batch_loop.set_postfix(tqdm_postfix)

            # Log batch details (less frequently)
            log_freq = max(1, loader_len // 10 if loader_len else 50) # Ensure log_freq > 0
            if (batch_idx + 1) % log_freq == 0 or (batch_idx + 1) == loader_len:
                 current_loss_log_l = running_loss_label / train_total_samples if train_total_samples > 0 else 0
                 current_acc_log_l = 100. * train_correct_label / train_total_samples if train_total_samples > 0 else 0
                 log_parts_batch = [f"{current_epoch_display:^7d}", f"{batch_idx+1}/{loader_len:^12}",
                                    f"{current_loss_log_l:^11.4f}", f"{current_acc_log_l:^10.1f}"]
                 if use_dann:
                     current_loss_log_d = running_loss_domain / train_total_samples if train_total_samples > 0 else 0
                     log_parts_batch.extend([f"{current_loss_log_d:^9.4f}", f"{grl_lambda_this_batch:^7.2f}"])
                 lr_value_str = f"{optimizer.param_groups[0]['lr']:.2e}"
                 log_parts_batch.extend([f"{'---':^10}", f"{'---':^10}", f"{lr_value_str:^8}", f"{'':^6}"])
                 write_log(" | ".join(log_parts_batch), use_tqdm_write=True)

        batch_loop.close()
        epoch_train_time = time.time() - epoch_start_time
        
        epoch_train_loss_label = running_loss_label / train_total_samples if train_total_samples > 0 else float('nan')
        epoch_train_acc_label = 100. * train_correct_label / train_total_samples if train_total_samples > 0 else float('nan')
        history_this_session['train_loss'].append(epoch_train_loss_label if np.isfinite(epoch_train_loss_label) else None)
        history_this_session['train_acc'].append(epoch_train_acc_label if np.isfinite(epoch_train_acc_label) else None)
        history_this_session['lr'].append(optimizer.param_groups[0]['lr'])

        if use_dann:
            epoch_train_loss_domain = running_loss_domain / train_total_samples if train_total_samples > 0 else float('nan')
            history_this_session['train_domain_loss'].append(epoch_train_loss_domain if np.isfinite(epoch_train_loss_domain) else None)
            avg_grl_lambda_epoch = np.mean(accumulated_grl_lambdas_epoch) if accumulated_grl_lambdas_epoch else 0.0
            history_this_session['grl_lambda_values'].append(avg_grl_lambda_epoch)
            history_this_session['dann_tradeoff_values'].append(dann_tradeoff_lambda)


        # --- Validation Phase ---
        epoch_val_loss, epoch_val_acc = None, None
        is_best_this_epoch = False
        epoch_val_time = 0.0

        if val_loader:
            val_start_time = time.time()
            model.eval()
            val_loss_running, val_correct_running, val_total_running = 0.0, 0, 0
            with torch.no_grad():
                val_loop = tqdm(val_loader, desc=f"Epoch {current_epoch_display}/{num_epochs} Val  ", leave=False, unit="batch")
                for batch_data_val in val_loop:
                    if batch_data_val is None: continue
                    try: # Unpack validation data (may or may not have domain_ids)
                        if len(batch_data_val) == 4: inputs_v, labels_v, _, _ = batch_data_val # (img, lbl, dom, meta)
                        elif len(batch_data_val) == 3: inputs_v, labels_v, _ = batch_data_val # (img, lbl, meta_or_dom)
                        elif len(batch_data_val) == 2: inputs_v, labels_v = batch_data_val    # (img, lbl)
                        else: raise ValueError(f"Val data: Expected 2, 3 or 4 items, got {len(batch_data_val)}")
                        inputs_v, labels_v = inputs_v.to(device, non_blocking=True), labels_v.to(device, non_blocking=True)
                    except (ValueError, TypeError) as e:
                        logger.error(f"Error unpack val batch E{current_epoch_display}: {e}. Skip."); continue
                    if inputs_v.size(0) == 0: continue

                    with torch.cuda.amp.autocast(enabled=amp_enabled, dtype=amp_dtype):
                        if hasattr(model, 'grl'): # Heuristic for DANNModel: call in label_only mode
                            label_preds_v = model(inputs_v, mode='label_only')
                        else: # Standard model
                            label_preds_v = model(inputs_v)
                        loss_v = criterion_label(label_preds_v, labels_v.long())

                    if torch.isfinite(loss_v): val_loss_running += loss_v.item() * inputs_v.size(0)
                    _, predicted_v = torch.max(label_preds_v.data, 1)
                    val_correct_running += (predicted_v == labels_v).sum().item()
                    val_total_running += labels_v.size(0)
            val_loop.close()
            
            epoch_val_loss = val_loss_running / val_total_running if val_total_running > 0 else float('inf')
            epoch_val_acc = 100. * val_correct_running / val_total_running if val_total_running > 0 else 0.0
            history_this_session['val_loss'].append(epoch_val_loss if np.isfinite(epoch_val_loss) else None)
            history_this_session['val_acc'].append(epoch_val_acc if np.isfinite(epoch_val_acc) else None)
            epoch_val_time = time.time() - val_start_time

            if val_total_running > 0 and epoch_val_loss is not None and np.isfinite(epoch_val_loss):
                 val_loss_log_str = f"{epoch_val_loss:.4f}"
                 write_log(f"Validation Summary: Loss={val_loss_log_str}, Acc={epoch_val_acc:.2f}%")
                 best_loss_str = f"{best_val_loss:.4f}" if np.isfinite(best_val_loss) else "inf"
                 if epoch_val_loss < best_val_loss:
                     write_log(f"*** Val loss improved from {best_loss_str} to {val_loss_log_str}. Saving best model for epoch {current_epoch_display}...")
                     best_val_loss = epoch_val_loss
                     is_best_this_epoch = True
                     best_epoch_this_session = current_epoch_display
                     epochs_no_improve = 0
                     try:
                         best_model_save_path = best_model_path_template.format(current_epoch_display)
                         state_to_save_best = {
                             'epoch': epoch, 'model_state_dict': model.state_dict(),
                             'optimizer_state_dict': optimizer.state_dict(),
                             'scheduler_state_dict': scheduler.state_dict() if scheduler else None,
                             'amp_state_dict': scaler.state_dict() if amp_enabled else None,
                             'best_val_loss': best_val_loss, 'epochs_no_improve': epochs_no_improve,
                             'history': resume_history if resume_history else history_this_session
                         }
                         torch.save(state_to_save_best, best_model_save_path)
                         logger.info(f"Best model checkpoint saved to {best_model_save_path}")
                     except Exception as e: write_log(f"ERROR saving best model checkpoint: {e}")
                 else:
                     epochs_no_improve += 1
                     if early_stopping_patience: write_log(f"Val loss did not improve from {best_loss_str}. Patience: {epochs_no_improve}/{early_stopping_patience}")
                 
                 if scheduler:
                     if isinstance(scheduler, ReduceLROnPlateau): scheduler.step(epoch_val_loss)
                     elif epoch >= getattr(scheduler, 'first_epoch_step', 0): # For schedulers like CosineAnnealingWarmRestarts
                          scheduler.step()
                 if early_stopping_patience is not None and epochs_no_improve >= early_stopping_patience:
                     early_stop_triggered = True
                     write_log(f"\n! EARLY STOPPING triggered after epoch {current_epoch_display}.")
            else: # val_total_running == 0 or invalid loss
                 write_log("--- Validation metrics could not be computed or were invalid. ---")
                 if scheduler and not isinstance(scheduler, ReduceLROnPlateau) and epoch >= getattr(scheduler, 'first_epoch_step', 0):
                     scheduler.step()
        else: # No val_loader
            write_log("--- No validation loader provided, skipping validation phase. ---")
            if scheduler and not isinstance(scheduler, ReduceLROnPlateau) and epoch >= getattr(scheduler, 'first_epoch_step', 0):
                scheduler.step()

        # --- Log Epoch Summary ---
        if epoch == start_epoch and start_epoch == 0: # Print header again if it was a fresh start
            header_parts_ep = [f"{'Epoch':^7}", f"{'Batch':^12}", f"{'LabelLoss':^11}", f"{'LabelAcc':^10}"]
            if use_dann: header_parts_ep.extend([f"{'DomLoss':^9}", f"{'GRL_L':^7}"])
            header_parts_ep.extend([f"{'ValLoss':^10}", f"{'ValAcc':^10}", f"{'LR':^8}", f"{'Best':^6}"])
            header_ep = " | ".join(header_parts_ep)
            separator_ep = "-" * len(header_ep)
            write_log(separator_ep); write_log(header_ep); write_log(separator_ep)
            
        train_loss_str = f"{epoch_train_loss_label:.4f}" if np.isfinite(epoch_train_loss_label) else "---"
        train_acc_str = f"{epoch_train_acc_label:.1f}" if np.isfinite(epoch_train_acc_label) else "---"
        
        log_epoch_parts = [
            f"{current_epoch_display:^7d}", f"{'Complete':^12}",
            f"{train_loss_str:^11}",
            f"{train_acc_str:^10}"
        ]
        if use_dann:
            domain_loss_str = f"{epoch_train_loss_domain:.4f}" if np.isfinite(epoch_train_loss_domain) else "---"
            grl_lambda_str = f"{avg_grl_lambda_epoch:.2f}" if np.isfinite(avg_grl_lambda_epoch) else "---"
            log_epoch_parts.extend([
                f"{domain_loss_str:^9}",
                f"{grl_lambda_str:^7}"
            ])
        
        val_loss_str = f"{epoch_val_loss:.4f}" if epoch_val_loss is not None and np.isfinite(epoch_val_loss) else "---"
        val_acc_str = f"{epoch_val_acc:.1f}" if epoch_val_acc is not None and np.isfinite(epoch_val_acc) else "---"
        lr_value_str = f"{optimizer.param_groups[0]['lr']:.2e}"
        
        log_epoch_parts.extend([
            f"{val_loss_str:^10}",
            f"{val_acc_str:^10}",
            f"{lr_value_str:^8}", f"{'*' if is_best_this_epoch else '':^6}"
        ])
        
        write_log(" | ".join(log_epoch_parts))
        epoch_total_time = time.time() - epoch_start_time
        write_log(f"Epoch {current_epoch_display} Timings: Train={epoch_train_time:.2f}s, Val={epoch_val_time:.2f}s, Total={epoch_total_time:.2f}s\n")

        # --- Save Last Checkpoint ---
        try:
            state_to_save_last = {
                'epoch': epoch, 'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict() if scheduler else None,
                'amp_state_dict': scaler.state_dict() if amp_enabled else None,
                'best_val_loss': best_val_loss, 'epochs_no_improve': epochs_no_improve,
                'history': resume_history if resume_history else history_this_session
            }
            torch.save(state_to_save_last, last_checkpoint_path)
        except Exception as e:
            write_log(f"ERROR saving last checkpoint {last_checkpoint_path}: {e}")

        if early_stop_triggered: break

    # --- Training Complete ---
    total_duration = time.time() - overall_start_time
    # `epoch` here is the last completed epoch (0-indexed)
    # If loop finished normally, epoch = num_epochs - 1. If early stopped, it's the epoch it stopped at.
    epochs_completed_this_run = (epoch - start_epoch + 1) if epoch >= start_epoch else 0
    
    final_msg = f"\nTraining finished for {experiment_name} after {epochs_completed_this_run} epochs this run (total epochs completed: {epoch + 1})."
    if early_stop_triggered: final_msg += " (Early stopping triggered)"
    final_msg += f"\nTotal duration for this run: {total_duration//60:.0f}m {total_duration%60:.0f}s"
    best_loss_str = f"{best_val_loss:.4f}" if np.isfinite(best_val_loss) else "N/A"
    
    final_best_epoch_overall = -1
    if resume_history and resume_history.get('val_loss'):
        # Combine past and current session validation losses for overall best
        combined_val_losses_all = resume_history['val_loss'] + [l for l in history_this_session['val_loss'] if l is not None]
        valid_combined_losses_all = [l for l in combined_val_losses_all if l is not None and np.isfinite(l)]
        if valid_combined_losses_all:
             final_best_val_loss_overall = min(valid_combined_losses_all)
             # Index will be 0-based, add 1 for 1-based epoch number
             final_best_epoch_overall = valid_combined_losses_all.index(final_best_val_loss_overall) + 1
             final_msg += f"\nOverall Best validation loss: {final_best_val_loss_overall:.4f} at overall epoch {final_best_epoch_overall}"
    elif best_epoch_this_session != -1: # Fresh run or no valid past history
        final_msg += f"\nBest validation loss for this run: {best_loss_str} at epoch {best_epoch_this_session}"
    else: # No best epoch found in this run (e.g., no validation)
        final_msg += f"\nBest validation loss for this run: {best_loss_str}"

    separator_end = "-" * 80
    write_log("=" * len(separator_end))
    write_log(final_msg)
    write_log("=" * len(separator_end))

    # --- Plot History ---
    # Plotting history for the current session + any resumed history
    history_plot_path = os.path.join(checkpoint_dir, f'{experiment_name}_training_history_epochs_{start_epoch + 1}_to_{epoch + 1}.png')
    if plot_training_history is not None:
        try:
            plot_hist_final = {}
            if resume_history: # Merge histories for plotting
                for key in set(resume_history.keys()) | set(history_this_session.keys()):
                    past_data = resume_history.get(key, [])
                    current_data = history_this_session.get(key, [])
                    # Ensure list concatenation, even if one is None or not list
                    plot_hist_final[key] = (past_data if isinstance(past_data, list) else []) + \
                                           (current_data if isinstance(current_data, list) else [])
            else:
                plot_hist_final = history_this_session

            # Check if there's any data to plot to avoid error with empty lists
            if any(plot_hist_final.get(k) for k in ['train_loss', 'val_loss', 'train_acc', 'val_acc', 'train_domain_loss']):
                plot_training_history(plot_hist_final, save_path=history_plot_path)
            else:
                logger.warning("No valid data in combined history to plot.")
        except Exception as plot_e:
            logger.error(f"Error generating training plot: {plot_e}", exc_info=True)
    else:
        logger.info("plot_training_history function not available. Skipping plot generation.")


    # --- Load Best Model Weights ---
    final_model_to_return = model # Default to model from last epoch of this run
    
    # Determine the path to the best model checkpoint based on overall best
    # This logic assumes best_model_path_template uses 1-based epoch numbers.
    path_to_load_best = None
    if final_best_epoch_overall != -1 : # Best epoch identified from combined history
        path_to_load_best = best_model_path_template.format(final_best_epoch_overall)
        logger.info(f"Overall best epoch was {final_best_epoch_overall}.")
    elif best_epoch_this_session != -1: # Best epoch from this session only
        path_to_load_best = best_model_path_template.format(best_epoch_this_session)
        logger.info(f"Best epoch for this session was {best_epoch_this_session}.")

    if path_to_load_best and os.path.exists(path_to_load_best):
        logger.info(f"Loading weights from best model checkpoint: {path_to_load_best}")
        try:
            checkpoint = torch.load(path_to_load_best, map_location=device)
            # Handle different checkpoint structures
            state_dict_key = 'model_state_dict'
            if state_dict_key not in checkpoint: # Try other common keys
                potential_keys = ['state_dict', 'model']
                for pk in potential_keys:
                    if pk in checkpoint:
                        state_dict_key = pk
                        break
                else: # If checkpoint itself is the state_dict
                    if all(isinstance(v, torch.Tensor) for v in checkpoint.values()):
                         state_dict_key = None # Indicates checkpoint is the state_dict itself


            if state_dict_key:
                model.load_state_dict(checkpoint[state_dict_key])
            elif state_dict_key is None and isinstance(checkpoint, dict): # Checkpoint is the state_dict
                 model.load_state_dict(checkpoint)
            else:
                logger.error(f"Could not find model state_dict in {path_to_load_best} using common keys.")

            final_model_to_return = model
        except Exception as e:
            logger.error(f"ERROR loading best model state_dict from {path_to_load_best}: {e}. Returning model from last epoch.", exc_info=True)
    elif path_to_load_best: # Path was determined but file doesn't exist
         logger.warning(f"Best model checkpoint ({path_to_load_best}) not found. Returning model from last epoch.")
    else: # No best model identified (e.g., no validation or error)
         logger.info("No specific best model checkpoint identified or saved. Returning model from last epoch.")

    return final_model_to_return, history_this_session